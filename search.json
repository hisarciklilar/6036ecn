[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R-workshops",
    "section": "",
    "text": "Welcome!\nThis workbook is created for the seminar sessions of\n6036ECN Further Econometrics module.\nIt is written using Quarto on RStudio by\nMehtap Hisarciklilar",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 R, R Studio and Quarto\nR is a very powerful statistical software that is becoming increasingly popular. Being able to do data analysis using R will very likely increase your employability.\nWarning: R is not like other apps that you have used! It requires coding. You will need to attend the seminar sessions and practice regularly. There will be a lot of struggle, but the result is worth it.\nR, as a programming language, is like any other language: the more you use it, the better you will get. Therefore, make sure to attend the lectures & seminars and engage with the module material. Otherwise, you will struggle to catch-up.\nRStudio has four main windows, that often have more than just one purpose. Figure 1.1 provides a brief description of each RStudio window. We will use all of them during the module, but the most important ones will be the console and the editor pane.",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro.html#r-r-studio-and-quarto",
    "href": "intro.html#r-r-studio-and-quarto",
    "title": "1  Introduction to R",
    "section": "",
    "text": "I list below three apps that you will need to work with this module’s material. I recommend installing these on your computers. Alternatively, you may use Coventry University’s Appsanywhere platform to get access.\nWe will be using R as the statistical analysis tool in this module. For R documentations, support and download links, visit the R Project for Statistical Computing. R is freely available for Linux, MacOS and Windows. Please download the version that matches your computer’s operating system.\nTo facilitate your work with R, I highly recommend to download and install the integrated development environment (IDE) RStudio Desktop from posit. This platform will make it easier for you to write and run R code.\nA final package that I highly recommend you to install is a publishing system, Quarto. You may use Quarto to produce documents in various formats (such as HTML, MS Word, PDF, PowerPoint, etc) while integrating your R code and output. You will easily have the option to change the format of your output as you desire. I will be using Quarto to produce R worksheets for this module. Please visit Quarto for further information and download.\n\nOnce you download Quarto, you will have access to it through RStudio.\n\n\n\n\n\n\n\n\n\nFigure 1.1: RStudio windows and their functions",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro.html#file-organisation",
    "href": "intro.html#file-organisation",
    "title": "1  Introduction to R",
    "section": "1.2 File Organisation",
    "text": "1.2 File Organisation\n\nCreate a folder for this module. This folder should include all module material you download from Aula or other platforms. Group files in sub-folders in a way that you can locate them easily. So for example, 6036ECN-Further-Econometrics may be the name of the folder and then you may have sub-folders such as Lecture-Slides, R-workshops, etc.\nYou should have one folder for R-workshops. I recommend naming this folder as R-workshops and within that folder, create sub-folders as we progress in the module.\nPlease note that my R-workshops folder is located on my desktop. Hence, I will refer to the folder as ~/Desktop/R-workshops. You will need to modify this depending on where you locate your files.\nIf you are using the computers in the lab, it may be best if you create a folder on your OneDrive account as you can easily access this at home and on-campus.\nBefore working on the data, set your working directory. R will save all files in there and, if you want to open a dataset, R will also look in there first. Select the folder you have created for R workshops.\nUse setwd(the_address_you_would_like_to_locate_your_work) in the console to choose your work directory. You may alternatively do this through the menu:\n\nSession –&gt; Set Working Directory –&gt; Choose Directory\nYou will see the console printing this action, which may help you to remember how to use the console next time.\n\nIf you are unsure of in which folder your work is, type getwd() in the console and R will print the current location you are at.",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro.html#getting-help",
    "href": "intro.html#getting-help",
    "title": "1  Introduction to R",
    "section": "1.3 Getting Help",
    "text": "1.3 Getting Help\nIf you should ever struggle with some of R’s commands, a look into R’s help-files can be very helpful. To access the help file, you have to type into the console window ? and then the command name. For example, if you want to know more about the command getwd(), type the following:\n\n?getwd()",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "basics-of-R.html",
    "href": "basics-of-R.html",
    "title": "2  Basics of R",
    "section": "",
    "text": "2.1 Using R as a calculator\nYou may use R as a calculator. Some examples are given below.\n# Addition\n5 + 4\n\n[1] 9\n# Subtraction\n5 - 4\n\n[1] 1\n# Multiplication\n3 * 6\n\n[1] 18\n# Division\n10 / 2\n\n[1] 5\n# Exponents\n2^3\n\n[1] 8\n# Modulo\n5 %% 2\n\n[1] 1",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basics-of-R.html#using-r-as-a-calculator",
    "href": "basics-of-R.html#using-r-as-a-calculator",
    "title": "2  Basics of R",
    "section": "",
    "text": "2.1.1 Basic Operators\n\n\n\nOperator\nDescription\n\n\n\n\nArithmetic\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^ or **\nExponential\n\n\n%%\nModulus\n\n\n% / %\nInteger Division\n\n\nLogic\n\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nGreater than\n\n\n&gt;=\nGreater than or equal to\n\n\n==\nExactly equal to\n\n\n!=\nNot equal to\n\n\n!x\nNot x\n\n\nx | y\nx OR y\n\n\nx & y\nx AND y\n\n\n\n\n\n2.1.2 Order of operators\n\nParenthesis\nMultiplication / division\nAddition / subtraction\nMultiplication has the same importance as division. Similarly, addition and subtraction are at the same level. When we need to decide between the two, we apply the operation that shows first from the left to the right.\nUse of parentheses makes it easier to perform the correct operation\nCan you guess the result of the following operation?\n\n8 / 2 * ( 2 + 2)\n\n\n\n8 / 2 * ( 2 + 2)\n\n[1] 16\n\n\n\n8 / 2 * 2 + 2\n\n[1] 10\n\n\n\n100 * 2 + 50 / 2\n\n[1] 225\n\n\n\n(100 * 2) + (50 / 2)\n\n[1] 225",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basics-of-R.html#storing-information-in-objects",
    "href": "basics-of-R.html#storing-information-in-objects",
    "title": "2  Basics of R",
    "section": "2.2 Storing information in objects",
    "text": "2.2 Storing information in objects\nR lets you save data by storing it inside an R object. An object is a name that you can use to call up stored data.\n\na &lt;- 5\n\na\n\n[1] 5\n\n\n\na + 2\n\n[1] 7\n\n\nIn the example above, we store value of 5 under object a. We then call the value stored under a and sum it with 2.\nNote the use of &lt; together with - . This representation (&lt;-) resembles a backward pointing arrow, and it assigns the value 2 to the object a.\n\nb_vector &lt;- 1:6\nb_vector\n\n[1] 1 2 3 4 5 6\n\n## [1] 1 2 3 4 5 6\n\nIn the above example, we create a vector, whose elements are numbers from 1 to 6 and store it under b_vector.\nWhen you create an object, the object will appear in the environment pane of RStudio (on the top right-hand-side of the R screen). This pane will show you all of the objects you’ve created since opening RStudio.\n\n2.2.1 Naming of objects\nNote the following;\n\nAn object name cannot start with a number (for example, 2var or 2_var)\nA name cannot use some special symbols, like ^, !, $, @, +, -, /, or * . You may use _\nR is case-sensitive, so name and Name will refer to different objects\nR will overwrite any previous information stored in an object without asking your confirmation. So, be careful while making changes.\nYou can see which object names you have already used by calling the function ls:\n\nls()\n\n[1] \"a\"        \"b_vector\"\n\n## [1] \"a\"        \"b_vector\"\n\n\n\n\n2.2.2 Naming conventions\nYou may see the following styles for naming of variables:\n\nCamel case\n\nCamel case variable naming is common in Javascipt. However, it is considered as bad practise in R. Try to avoid this kind of naming.\n\nbankAccount = 100\n\n\nUse of dots\n\ndot is used in variable names by many R users. However, try to avoid this too because base R uses dots in function names (contrib.url()) and class names (data.frame). Avoiding dot in your variable names will help you avoid confusion, particularly in the initial stages of your learning!\n\nbank.account = 100\n\n\nSnake case\n\nUse of snake case is considered to be good practice. Try to follow this approach.\n\nbank_account = 100\n\nNote that you may find different users of R having a preference towards different styles. The recommendations above are from the “Tidyverse style guide”, which is available from https://style.tidyverse.org.\nStart your variable names with a lower case and reserve the capital letter start for function names!\n\n\n2.2.3 Removing objects\nYou will see that the Environment window can quickly get over-crowded while working interactively. You may remove the objects that you no longer need. by rm(object_name )\n\nrm(a)\n\n\n\n2.2.4 Example of using variables\nLet us calculate the module mark for a student who got 65% from coursework and 53% from exam. The weights for the coursework and exam are, respectively, 25% and 75%.\n\n# let's calculate module mark for a student\ncoursework &lt;- 65\nexam &lt;- 53\nmodule_mark &lt;- coursework * 0.25 + exam * 0.75\n\nprint(module_mark)\n\n[1] 56",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "basics-of-R.html#datatypes-in-r",
    "href": "basics-of-R.html#datatypes-in-r",
    "title": "2  Basics of R",
    "section": "2.3 Datatypes in R",
    "text": "2.3 Datatypes in R\n\n2.3.1 Numeric\nDecimal numbers and integers are part of the numeric class in R.\n\n2.3.1.1 Decimal (floating point values)\n\ndecimal_number &lt;- 2.2\n\n\n\n2.3.1.2 Integer\n\ni &lt;- 5\n\n\n\n\n2.3.2 Logical\nBoolean values (TRUE and FALSE) are part of the logical class in R. These are written in capital letters.\n\nt &lt;- TRUE\nf &lt;- FALSE\n\n\nt\n\n[1] TRUE\n\n\n\nf\n\n[1] FALSE\n\n\n\n\n2.3.3 Characters\nText (string) values are known as characters in R. You may use single or double quotation to create a text (string).\n\nmessage &lt;- \"hello all!\"\nprint(message)\n\n[1] \"hello all!\"\n\n\n\nan_other_message &lt;- 'how are you?'\nprint(an_other_message)\n\n[1] \"how are you?\"\n\n\n\n\n2.3.4 Checking data type classes\nWe can use the class() function to check the data type of a variable:\n\nclass(decimal_number)\n\n[1] \"numeric\"\n\n\n\nclass(i)\n\n[1] \"numeric\"\n\n\n\nclass(t)\n\n[1] \"logical\"\n\n\n\nclass(f)\n\n[1] \"logical\"\n\n\n\nclass(message)\n\n[1] \"character\"",
    "crumbs": [
      "Seminar 1 (21 January 2025)",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "data-management-in-R.html",
    "href": "data-management-in-R.html",
    "title": "3  Data Management in R",
    "section": "",
    "text": "3.1 Packages and libraries\nIn order to access specialised data analysis tools in R, we will need to install some R packages.\n“An R package is a collection of functions, data, and documentation that extends the capabilities of base R. Using packages is key to the successful use of R.” (Wickham, Cetinkaya-Rundel, and Grolemund, n.d.)\nWe will start by installing the tidyverse package\n#install.packages(\"tidyverse\")\nTo install tidyverse, type the above line of code in the console, and then press enter to run it. R will download the packages from CRAN and install them on to your computer.\nOnce installed, you may use this package after loading it with the library() function.\n#library(tidyverse)\nYou see above a list of packages that come with tidyverse.\nYou may update tidyverse by running\n#tidyverse_update()",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management in R</span>"
    ]
  },
  {
    "objectID": "data-management-in-R.html#functions",
    "href": "data-management-in-R.html#functions",
    "title": "3  Data Management in R",
    "section": "3.2 Functions",
    "text": "3.2 Functions\nYou may identify functions with the () after the function name. For example, ls() that we used above.\nFunctions may also take arguments. The data that we pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function.\n\n# round a number\nround(4.5218)\n\n[1] 5\n\n## 5\n\n# calculate the factorial\nfactorial(3)\n\n[1] 6\n\n## 6\n\n# calculate the mean of values from 1 to 6:\nmean(1:6)\n\n[1] 3.5\n\n## 3.5\n\nround(mean(1:6))\n\n[1] 4\n\n## 4\n\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nTo see which arguments a function can take, you may type args in parenthesis after function name:\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n## function (x, digits = 0) \n## NULL\n\nround(3.1415, digits = 2)\n\n[1] 3.14\n\n## 3.14\n\n\n3.2.1 Basic Functions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\n?() or help()\nAccess the documentation and help file for a particular function\n\n\ninstall.packages()\nDownload and install an R package\n\n\nlibrary()\nLoads an R package into the working environment\n\n\nsetwd()\nSet the working directory\n\n\ngetwd()\nGet the working directory\n\n\nc()\nCreate a vector\n\n\nas.numeric()\nConverts an object to a numeric vector\n\n\nas.logical()\nConverts an object to a logical vector\n\n\nas.character()\nConverts and object to a character vector\n\n\nmode()\nReturns the type of the object\n\n\nsum()\nReturns the sum of all input values\n\n\nlength()\nReturns the lenght of the obejct\n\n\nmean()\nReturns the arithmetic mean of the vector\n\n\nmedian()\nReturns the median of the vector\n\n\nsample()\nReturns a specificed size of elements from the object\n\n\nreplicate()\nRepeats an expression a specific number of times\n\n\nhist()\nCreates a histogram of given data values",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management in R</span>"
    ]
  },
  {
    "objectID": "data-management-in-R.html#scripts",
    "href": "data-management-in-R.html#scripts",
    "title": "3  Data Management in R",
    "section": "3.3 Scripts",
    "text": "3.3 Scripts\nYou can create a draft of your code as you go by using an R script. An R script is just a plain text file that you save R code in. You can open an R script in RStudio using the menu bar:\nFile –&gt; New File –&gt; R Script\nWe will write and edit R code in a script. This will help create a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day.\nTo save a script, click the scripts pane, and then go to File –&gt; Save As in the menu bar.\n\nYou can automatically execute a line of code in a script by clicking the Run button on the top right of the pane. R will run whichever line of code your cursor is on.\nIf you have a whole section highlighted, R will run the highlighted code.\nYou can run the entire script by clicking the Source button.\nYou can use Control + Return in your keyboard as a shortcut for the Run button. On Macs, that would be Command + Return.\n\n\n\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. n.d. R for Data Science. 2nd edition. O’Reilly. https://r4ds.hadley.nz/preface-2e.",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Management in R</span>"
    ]
  },
  {
    "objectID": "importing-data-into-R.html",
    "href": "importing-data-into-R.html",
    "title": "4  Importing Data into R",
    "section": "",
    "text": "4.1 Example 1: Crime data\nThe example and instructions provided in this section is taken from (Riegler 2022).\nThe following exercise gives you a hands-on introduction to basic operations in R using a real-world data set. It begins with importing a MS-Excel data set into R and asks you to perform some basic operations to familiarise yourself with some of the commands that will be relevant for the coursework and in subsequent computer classes.\nPlease download the Excel data set called crime.xls from Aula and save it into a drive of your choice. This is a data set that contains crime levels and other socio-economic information on 46 cities across the US for the year 1982. The full version of the data set can be accessed at http://fmwww.bc.edu/ec-p/data/wooldridge/datasets.list.html. The variables are defined as follows:\nFrom here on, you need to open a R script to save all your commands to be able to replicate your results:",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data into R</span>"
    ]
  },
  {
    "objectID": "importing-data-into-R.html#example-1-crime-data",
    "href": "importing-data-into-R.html#example-1-crime-data",
    "title": "4  Importing Data into R",
    "section": "",
    "text": "Variable\nDefinition\n\n\n\n\npop\nactual population in number\n\n\ncrimes\ntotal number of crimes\n\n\nunem\nunemployment rate (%)\n\n\nofficers\nnumber of police officers\n\n\npcinc\nper capita income, $\n\n\narea\nland area, square miles\n\n\nlawexpc\nlaw enforcement expenditure per capita, $\n\n\n\n\n\n4.1.1 Task 1\n\n4.1.1.1 Task\nImport the Excel data set into R.\n\n\n4.1.1.2 Guidance\nThe native data format of R is .Rdata, however, you can also open other formats, such as .xlsx, .csv, etc. Non-native data formats have to be imported rather than just opened. Before we can we can import Excel spreadsheets directly into R, we have to activate a R-library first.\nYou can either use the package manager window (in the right bottom corner of the R screen) and tick the box next to the package name or you type the following into the terminal window (in the left bottom of the R screen)\n\nlibrary(readxl)\n\nThis line loads the necessary readxl library. But you will probably receive an error message when you run the above line. This is because we first need to install the read_excel package. (Note that you will need to type the below line without the pound (hashtag) sign at the beginning of the line).\n\n# install.packages(\"readxl\")\nlibrary(readxl)\n\nThere are two ways to import:\n\nThrough command line:\n\ncrime &lt;- read_excel(\"./assets/data/crime.xls\")\n\nIn the above line, we import the dataset with the read_excel function and store it under the name crime. Notice how the new crime data is added as an object in the R environment.\nThrough menu:\nFile –&gt; Import Dataset –&gt; From Excel\n\nDon’t forget to tick the “First Row as Names” box if it is not ticked!\n\n\n\n4.1.2 Task 2\n\n4.1.2.1 Task\nView the dataset in R’s data viewer.\n\n\n4.1.2.2 Guidance\nTo open the data viewer, use the View function.\n\n# View(crime)\n\nNote that the first letter of View is capitalised.\n\n\n\n4.1.3 Task 3\n\n4.1.3.1 Task\nView the first few (six) entries of the crime data to get a feeling of what the values look like.\n\n\n4.1.3.2 Guidance\nUse the head function\n\nhead(crime)\n\n# A tibble: 6 × 7\n     pop crimes  unem officers pcinc  area lawexpc\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 229528  17136  8.20      326  8532  44.6    851.\n2 814054  75654  8.10     1621  7551 375      875.\n3 374974  31352  9         633  8343  49.8   1122.\n4 176496  15698 12.6       245  7592  74      744.\n5 288446  31202 12.6       504  7558  97.3    974.\n6 122768  16806 13.9       186  6411  55.3    762.\n\n\n\n\n\n4.1.4 Task 4\n\n4.1.4.1 Task\nLabel the variables using the definitions given above.\n\n\n4.1.4.2 Guidance\nYou have to attach a variable label to each variable. There is already a library available which facilitates the allocation of labels to variables. First, we need to install the package!\n\n# install.packages(\"expss\")\nlibrary(expss)\n\nLoading required package: maditr\n\n\n\nTo drop variable use NULL: let(mtcars, am = NULL) %&gt;% head()\n\ncrime &lt;- apply_labels(crime, \n                      pop = \"actual population in number\",\n                      crimes = \"total number of crimes\",\n                      unem = \"unemployment rate (%)\", \n                      officers = \"number of police officers\", \n                      pcinc = \"per capita income, $\",\n                      area = \"land area, square miles\", \n                      lawexpc = \"law enforcement expenditure per capita, $\"\n                      )\n\n\n\n\n4.1.5 Task 5\n\n4.1.5.1 Task\nCreate a new variable which measures the population density for each city.\n\n\n4.1.5.2 Guidance\nTo generate a new variable and add it to the existing crime dataset, we use the following command:\n\ncrime$popdens &lt;- crime$pop / crime$area\n\nYou may wonder why we add crime$ in front of every variable. The reason is that R can store more than one data frame, matrix, list, vector etc., at the same time, so the prefix crime$ is necessary to avoid ambiguity and ensure that we are working with variables in the crime data. Think of crime$ as an address where e.g. the variable pop stays. If you have loaded another data frame that contains a pop variable, R would know that we want to use the variable from the crime dataset and not from the other data frame. There are library packages that can facilitate the process, however, we will cover them later in the module.\nNote that the newly created population density variable is now labelled as the original population variable (pop). Let’s update the label with the method we introduced in Task 4. Note that we do not need to call the library again, as it is already called.\n\ncrime &lt;- apply_labels(crime, \n                      popdens = \"population density: pop / area\")\n\n\n\n\n4.1.6 Task 6\n\n4.1.6.1 Task\nSort the data with respect to the population density of each city.\n\n\n4.1.6.2 Guidance\nSorting data is a useful action to get a general feeling for the data, e.g. are there any outliers in the dataset? Are there any unusual patterns?\nTo change the order of the rows in a data frame, we will apply the order function. We first rank all observations with respect to the population density and store this information in a vector called rank. The rank vector contains indices that we can use to sort the crime data frame. Below, we save the sorted data under a new name, crime.popdens1\n\nrank &lt;- order(crime$popdens)\ncrime.popdens1 &lt;- crime[rank,]\n\nLet’s see the result (note. how the population density variable is now sorted from the smallest to the largest):\n\nhead(crime.popdens1)\n\n# A tibble: 6 × 8\n  pop        crimes     unem       officers   pcinc      area  lawexpc popdens  \n  &lt;labelled&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;lab&gt; &lt;label&gt; &lt;labelle&gt;\n1 425093     38195       4.7        767       7991       604.0  570.00  703.7964\n2 268887     14537       5.5        400       7704       255.9  570.63 1050.7503\n3 462657     34736      10.4        937       7585       352.0  582.56 1314.3665\n4 451397     45503      10.4       1145       7480       316.4 1054.17 1426.6656\n5 412661     47128       8.3        719       7336       258.5  554.70 1596.3675\n6 173630     18915       8.7        366       7409       100.5  827.16 1727.6617\n\n# you may alternatively use \n# View(crime.popdens1)\n\nThis procedure sorts the data from the smallest to the largest value. To sort the data from the largest to the smallest number, we set the order argument decreasing to TRUE.\n\ncrime.popdens2 &lt;- crime[order(crime$popdens, decreasing = TRUE),]\nhead(crime.popdens2)\n\n# A tibble: 6 × 8\n  pop        crimes     unem       officers   pcinc      area  lawexpc popdens  \n  &lt;labelled&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;lab&gt; &lt;label&gt; &lt;labelle&gt;\n1  708287     68598      8.4       1971       9265        46.4 1050.00 15264.806\n2  334414     36172     15.4       1166       4525        24.1 1139.32 13876.099\n3  365506     52901     12.3        979       6084        34.3  714.00 10656.152\n4 1181868    152962     20.3       4092       6251       135.6 1483.52  8715.840\n5  360493     28592     16.9       1034       5929        41.8  749.44  8624.235\n6  158533     15233     11.3        408       6169        18.9  661.50  8387.990\n\n\nHave you observed a slight difference in the way we sorted the data? We can save some time and space by merging the two steps into one line, however, it is sometimes easier to understand a command if it is split into separate stages.\n\n\n\n4.1.7 Task 7\n\n4.1.7.1 Task\nWhat is the minimum and maximum value for population density in the crime data?\n\n\n4.1.7.2 Guidance\nThe minimum and maximum values can be produced by generating standard descriptive statistics of the variables.\n\nsummary(crime$popdens)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  703.8  2797.1  4236.8  4967.5  7052.2 15264.8 \n\n\nBefore you finish, save the dataset under a new name. Never overwrite your original data!\n\nsave(crime, file = \"./assets/data/crime_v2.Rdata\")\n\nThe above command tells R to use the crime dataset and save it as crime_v2.Rdata. Rdata is an R specific format. R can also save data in .csv format, that can be opened with any text editor or spreadsheet software:\n\nwrite.csv(crime, file = \"./assets/data/crime_v2.csv\", row.names = TRUE) \n\nNow you are ready to answer the following questions on your own:\n\n\n\n4.1.8 Further Exercises\n\nFind the minimum and maximum number of police officers in the data set.\nCreate a new variable which measures the crime rate per 1,000 of population.\nIs the city with the highest number of police officers also the city with the highest crime density?\nHow many crimes occurred in the richest city?\nIs the richest city also the one with the highest number of police officers?\nWhat is the average unemployment rate across these 46 U.S. cities?\nDoes the city with the highest unemployment rate also have the highest crime level?\n\n\n\n\n\nRiegler, Robert. 2022. “R Workbook - Guidance for Worksheets.” Aston University.",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data into R</span>"
    ]
  },
  {
    "objectID": "introduction-to-regression-analysis.html",
    "href": "introduction-to-regression-analysis.html",
    "title": "5  Introduction to Regression Analysis",
    "section": "",
    "text": "5.1 Example 1: Crime data\nThe example and instructions provided in this section is taken from (Riegler 2022).\nSuppose you are examining the relationship between number of crimes and number of police officers. Below, we will generate descriptive statistics, create a scatter plot and see how we estimate OLS regression.\nWe will use the crime data set, which is already saved in Rdata format.",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Regression Analysis</span>"
    ]
  },
  {
    "objectID": "introduction-to-regression-analysis.html#example-1-crime-data",
    "href": "introduction-to-regression-analysis.html#example-1-crime-data",
    "title": "5  Introduction to Regression Analysis",
    "section": "",
    "text": "5.1.1 Task 1\nOpen crime_v2.Rdata (if it not already open). You may so this trough the menu or the command line using the load() function:\n\nload(\"~/Desktop/R-workshops/assets/data/crime_v2.Rdata\")\n\n\n\n5.1.2 Task 2\nCheck the summary statistics for crimes and officers variables.\n\nsummary(crime[, c(\"crimes\", \"officers\")])\n\n     crimes          officers     \n Min.   :  5276   Min.   : 109.0  \n 1st Qu.: 19658   1st Qu.: 402.8  \n Median : 32518   Median : 694.5  \n Mean   : 38123   Mean   : 902.1  \n 3rd Qu.: 49434   3rd Qu.:1212.0  \n Max.   :152962   Max.   :4092.0  \n\n\n\n# Standard deviation for the 'crimes' variable\nsd_crimes &lt;- sd(crime$crimes, na.rm = TRUE)\n\n# Standard deviation for the 'officers' variable\nsd_officers &lt;- sd(crime$officers, na.rm = TRUE)\n\n# Print the results\nsd_crimes\n\n[1] 27660.3\n\nsd_officers\n\n[1] 721.7255\n\n\nNote the na.rm = TRUE above. This argument ensures that nay NA values are removed before the calculation.\n\n\n5.1.3 Task 3\nIn addition to checking summary statistics, it is always wise to visualise your data before getting into more complicated modelling.\nFor this task, generate a scatter plot with number of crimes on the y-axis and the number of police officers on the x-axis.\n\nplot(crime$officers~crime$crimes, \n     main = \"Relationship between number of police officers and crime\")\n\n\n\n\n\n\n\n\n\n\n5.1.4 Task 4\nCalculate the Covariance and the Correlation Coefficient between number of crimes and number of police officers. Comment on their values.\n\n5.1.4.1 Guidance\nA scatter plot is a good start for identifying relationships between two variables, but it is not sufficient to identify accurately how strong the relationship is between crimes and officers. There are two numerical statistics, that provide information about the relationship between two variables: The Covariance and the Correlation Coefficient.\nTo produce a Covariance matrix, use the following command:\n\ncov(crime$officers, crime$crimes)\n\n[1] 18212436\n\n\nThe result is: 18,212,436! This number may appear to be too large but the value we obtain as covariance depends on the measuremetn levels of the variables. This measure (i.e. the covariance) does not provide any information on how strong this relationship between crimes and officers is. It only reveals that there is a positive relationship between the number of police officers and the number of crimes committed.\nInstead of using the covariance, we can use a standardised covariance - the correlation coefficient. To calculate the correlation matrix, we only have to adjust slightly the covariance command.\n\ncor(crime$officers, crime$crimes)\n\n[1] 0.9123032\n\n\nThe correlation coefficient between number of officers and crimes is 0.91. We conclude that there is a strong positive relationship between our two variables.\n\n\n\n5.1.5 Task 5\nRegress the number of police officers on crimes and comment on:\n\nthe sign and magnitude of the regression coefficients\nthe goodness of fit of the estimated model.\n\n\nlm(officers ~ crimes, data = crime)\n\n\nCall:\nlm(formula = officers ~ crimes, data = crime)\n\nCoefficients:\n(Intercept)       crimes  \n    -5.4183       0.0238  \n\n\nWe can save this estimation under an object (please note that we use model_1 below, but you may give any name as long as it satisfies the naming conventions):\n\nmodel_1 &lt;- lm(officers ~ crimes, data = crime)\n# display the model\nmodel_1\n\n\nCall:\nlm(formula = officers ~ crimes, data = crime)\n\nCoefficients:\n(Intercept)       crimes  \n    -5.4183       0.0238  \n\n\nAlthough we see the estimated coefficients in the above output we do not have information about the other statistics that we need to proceed. We use the summary() function below:\n\nsummary(model_1)\n\n\nCall:\nlm(formula = officers ~ crimes, data = crime)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-756.64 -153.71  -25.75   89.64 1000.97 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.418291  75.587257  -0.072    0.943    \ncrimes       0.023804   0.001611  14.777   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 298.9 on 44 degrees of freedom\nMultiple R-squared:  0.8323,    Adjusted R-squared:  0.8285 \nF-statistic: 218.4 on 1 and 44 DF,  p-value: &lt; 2.2e-16\n\n\nThe intercept term is not statistically significant.\nThe crimes variable is statistically significant at 0.1%. (Note the significance codes in the output).\nThe slope coefficient states that for every additional crime, we observe on average of 0.024 more police officers. Using more reader-friendly numbers, we can also infer that for every 1,000 additional crimes committed within a city, 24 more police officers are employed. Note how the latter way of phrasing makes more sense.\n\\(R^2\\) is the measure that provides information on the overall goodness of fit of the model. In this case it is 0.83. This means that 83% of the variation in police officers can be explained with the variation in number of crimes committed. Our estimated model has a good degree of explanatory power.\nLooking at the F-statistic (218.4 with a p-value of almost zero), we can conclude that the model, overall, is statistically significant.\n\n\n5.1.6 Task 6\nAdd a regression line to the scatter plot you created in Task 3.\n\n5.1.6.1 Guidance\nTo add a regression line to the plot, we have to use the previously saved regression object model_1 and add it to the previous scatter plot.\n\nplot(crime$officers~crime$crimes, \n     main = \"Relationship between number of police officers and crime\")\nabline(model_1)",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Regression Analysis</span>"
    ]
  },
  {
    "objectID": "introduction-to-regression-analysis.html#example-2-wage-data",
    "href": "introduction-to-regression-analysis.html#example-2-wage-data",
    "title": "5  Introduction to Regression Analysis",
    "section": "5.2 Example 2: Wage data",
    "text": "5.2 Example 2: Wage data\n\n5.2.1 Task 1\n\n5.2.1.1 Task\nImport wage.xls data into R and view the first few rows of the data to have an idea about the contents of the variables, and then save the data in R format.\n\n\n5.2.1.2 Guidance\nUse read_excel() and head() functions.\n\n# install.packages(\"readxl\")\nlibrary(readxl)\n\n# Import Excel data\nwage2 &lt;- read_excel(\"./assets/data/wage2.xls\", sheet = \"wage2\")\n\n\nhead(wage2)\n\n# A tibble: 6 × 15\n   wage hours    IQ   KWW  educ exper tenure   age married south urban  sibs\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   769    40    93    35    12    11      2    31       1     0     1     1\n2   808    50   119    41    18    11     16    37       1     0     1     1\n3   825    40   108    46    14    11      9    33       1     0     1     1\n4   650    40    96    32    12    13      7    32       1     0     1     4\n5   562    40    74    27    11    14      5    34       1     0     1    10\n6  1400    40   116    43    16    14      2    35       1     0     1     1\n# ℹ 3 more variables: brthord &lt;dbl&gt;, meduc &lt;dbl&gt;, feduc &lt;dbl&gt;\n\n\n\n# Save data in R format\nsave(wage2, file = \"./assets/data/wage2.Rdata\")\n\n\n\n\n5.2.2 Task 2\n\n5.2.2.1 Task\nLabel variable educ as “years of schooling” and exper as “years of experience”.\n\n\n5.2.2.2 Guidance\nWe will need the expss package to label the variables. The installation and calling of the package is deactivated below since we already have done these steps above. After running the below command check the changes in the data from the Environment window on the top-right.\n\n# install.packages(\"expss\")\nlibrary(expss)\n\nLoading required package: maditr\n\n\n\nTo aggregate data: take(mtcars, mean_mpg = mean(mpg), by = am)\n\n\n\nUse 'expss_output_viewer()' to display tables in the RStudio Viewer.\n To return to the console output, use 'expss_output_default()'.\n\nwage2 &lt;- apply_labels(wage2, \n                      educ = \"years of schooling\", \n                      exper = \"years of experince\")\n\n\n\n\n5.2.3 Task 3\n\n5.2.3.1 Task\nGenerate two new variables: hourly wage and logarithmic wage.\n\n\n5.2.3.2 Guidance\n\n# Generate new variables\nwage2$hourly_wage &lt;- wage2$wage / wage2$hours\nwage2$ln_wage &lt;- log(wage2$wage)\n\n\n\n\n5.2.4 Task 4\n\n5.2.4.1 Task\nCheck the summary statistics for (i) the wage variable, (ii) for all variables.\n\n\n5.2.4.2 Guidance\nWe will use the summary() function.\n\n# Summary statistics for the wage variable only\nsummary(wage2$wage)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  115.0   669.0   905.0   957.9  1160.0  3078.0 \n\n\n\n# Summary statistics for all variables in wage2 data\nsummary(wage2)\n\n      wage            hours             IQ             KWW       \n Min.   : 115.0   Min.   :20.00   Min.   : 50.0   Min.   :12.00  \n 1st Qu.: 669.0   1st Qu.:40.00   1st Qu.: 92.0   1st Qu.:31.00  \n Median : 905.0   Median :40.00   Median :102.0   Median :37.00  \n Mean   : 957.9   Mean   :43.93   Mean   :101.3   Mean   :35.74  \n 3rd Qu.:1160.0   3rd Qu.:48.00   3rd Qu.:112.0   3rd Qu.:41.00  \n Max.   :3078.0   Max.   :80.00   Max.   :145.0   Max.   :56.00  \n                                                                 \n      educ           exper           tenure            age       \n Min.   : 9.00   Min.   : 1.00   Min.   : 0.000   Min.   :28.00  \n 1st Qu.:12.00   1st Qu.: 8.00   1st Qu.: 3.000   1st Qu.:30.00  \n Median :12.00   Median :11.00   Median : 7.000   Median :33.00  \n Mean   :13.47   Mean   :11.56   Mean   : 7.234   Mean   :33.08  \n 3rd Qu.:16.00   3rd Qu.:15.00   3rd Qu.:11.000   3rd Qu.:36.00  \n Max.   :18.00   Max.   :23.00   Max.   :22.000   Max.   :38.00  \n                                                                 \n    married          south            urban             sibs       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   : 0.000  \n 1st Qu.:1.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 1.000  \n Median :1.000   Median :0.0000   Median :1.0000   Median : 2.000  \n Mean   :0.893   Mean   :0.3412   Mean   :0.7176   Mean   : 2.941  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 4.000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :14.000  \n                                                                   \n    brthord           meduc           feduc        hourly_wage    \n Min.   : 1.000   Min.   : 0.00   Min.   : 0.00   Min.   :  2.30  \n 1st Qu.: 1.000   1st Qu.: 8.00   1st Qu.: 8.00   1st Qu.: 15.07  \n Median : 2.000   Median :12.00   Median :10.00   Median : 21.02  \n Mean   : 2.277   Mean   :10.68   Mean   :10.22   Mean   : 22.32  \n 3rd Qu.: 3.000   3rd Qu.:12.00   3rd Qu.:12.00   3rd Qu.: 27.70  \n Max.   :10.000   Max.   :18.00   Max.   :18.00   Max.   :102.60  \n NA's   :83       NA's   :78      NA's   :194                     \n    ln_wage     \n Min.   :4.745  \n 1st Qu.:6.506  \n Median :6.808  \n Mean   :6.779  \n 3rd Qu.:7.056  \n Max.   :8.032  \n                \n\n\n\n\n\n5.2.5 Task 5\n\n5.2.5.1 Task\nCalculate the correlation coefficient between wage and education.\n\n\n5.2.5.2 Guidance\nWe can calculate the correlation coefficients using the cor() function. In the first example below, the correlation coefficient is reported as a single number, while in the second example, we get a correlation matrix.\nIn most of empirical work, we are usually interested with pairwise correlations among all variables. Hence, we may use the correlation matrix to check the binary correlations among all variables in our sample. This is provided in the third example below.\nThe \"use = complete.obs\" added to the commands below asks R to handle missing values by casewise deletion.\n\n# Correlation\ncor(wage2$wage, wage2$educ)\n\n[1] 0.3271087\n\n\n\n# Correlation\ncor(wage2[, c(\"wage\", \"educ\")], use = \"complete.obs\")\n\n          wage      educ\nwage 1.0000000 0.3271087\neduc 0.3271087 1.0000000\n\n\n\ncor(wage2, use = \"pairwise.complete.obs\")\n\n                    wage        hours          IQ         KWW        educ\nwage         1.000000000 -0.009504302  0.30908783  0.32613058  0.32710869\nhours       -0.009504302  1.000000000  0.07383930  0.11388938  0.09100889\nIQ           0.309087827  0.073839301  1.00000000  0.41351552  0.51569701\nKWW          0.326130577  0.113889381  0.41351552  1.00000000  0.38813424\neduc         0.327108690  0.091008888  0.51569701  0.38813424  1.00000000\nexper        0.002189702 -0.062126227 -0.22491253  0.01745245 -0.45557312\ntenure       0.128266391 -0.055528006  0.04215883  0.14139800 -0.03616655\nage          0.156701761  0.024811636 -0.04374091  0.39305297 -0.01225396\nmarried      0.136582670  0.032563350 -0.01466753  0.08994782 -0.05856602\nsouth       -0.159387287 -0.029519177 -0.20978466 -0.09439242 -0.09703298\nurban        0.198406472  0.016573046  0.03893553  0.09819025  0.07215091\nsibs        -0.159203728 -0.049602555 -0.28477277 -0.28497534 -0.23928810\nbrthord     -0.145485385 -0.043129582 -0.17943947 -0.15358472 -0.20499246\nmeduc        0.214831839  0.076619806  0.33180383  0.24079168  0.36423913\nfeduc        0.237586922  0.063172297  0.34390758  0.23488927  0.42692545\nhourly_wage  0.931240501 -0.317645466  0.26502635  0.26059936  0.27167136\nln_wage      0.953141156 -0.047219079  0.31478770  0.30627128  0.31211665\n                   exper      tenure          age      married       south\nwage         0.002189702  0.12826639  0.156701761  0.136582670 -0.15938729\nhours       -0.062126227 -0.05552801  0.024811636  0.032563350 -0.02951918\nIQ          -0.224912532  0.04215883 -0.043740911 -0.014667528 -0.20978466\nKWW          0.017452446  0.14139800  0.393052967  0.089947816 -0.09439242\neduc        -0.455573115 -0.03616655 -0.012253956 -0.058566019 -0.09703298\nexper        1.000000000  0.24365440  0.495329763  0.106349115  0.02125724\ntenure       0.243654402  1.00000000  0.270601647  0.072605374 -0.06169141\nage          0.495329763  0.27060165  1.000000000  0.106980249 -0.02947768\nmarried      0.106349115  0.07260537  0.106980249  1.000000000  0.02275672\nsouth        0.021257241 -0.06169141 -0.029477681  0.022756718  1.00000000\nurban       -0.047385845 -0.03848582 -0.006749288 -0.040248179 -0.10989797\nsibs         0.064310470 -0.03916116 -0.040719238 -0.004327422  0.06631979\nbrthord      0.088300019 -0.02847775  0.005435916 -0.014737189  0.09370679\nmeduc       -0.186317286 -0.01496769 -0.029319099 -0.022763437 -0.15787359\nfeduc       -0.256792630 -0.05924123 -0.071303285 -0.020324390 -0.17236334\nhourly_wage  0.017757793  0.13541822  0.126683019  0.115115701 -0.14716118\nln_wage      0.020601158  0.18585262  0.161822314  0.149975894 -0.19481092\n                   urban         sibs      brthord       meduc       feduc\nwage         0.198406472 -0.159203728 -0.145485385  0.21483184  0.23758692\nhours        0.016573046 -0.049602555 -0.043129582  0.07661981  0.06317230\nIQ           0.038935525 -0.284772765 -0.179439471  0.33180383  0.34390758\nKWW          0.098190247 -0.284975345 -0.153584717  0.24079168  0.23488927\neduc         0.072150908 -0.239288104 -0.204992462  0.36423913  0.42692545\nexper       -0.047385845  0.064310470  0.088300019 -0.18631729 -0.25679263\ntenure      -0.038485824 -0.039161158 -0.028477749 -0.01496769 -0.05924123\nage         -0.006749288 -0.040719238  0.005435916 -0.02931910 -0.07130328\nmarried     -0.040248179 -0.004327422 -0.014737189 -0.02276344 -0.02032439\nsouth       -0.109897970  0.066319792  0.093706790 -0.15787359 -0.17236334\nurban        1.000000000 -0.031468824  0.002419787  0.03402366  0.11223944\nsibs        -0.031468824  1.000000000  0.593913799 -0.28715120 -0.23202649\nbrthord      0.002419787  0.593913799  1.000000000 -0.27593376 -0.23037060\nmeduc        0.034023660 -0.287151198 -0.275933760  1.00000000  0.57649476\nfeduc        0.112239438 -0.232026494 -0.230370600  0.57649476  1.00000000\nhourly_wage  0.189240304 -0.131364072 -0.120293460  0.18348733  0.20469678\nln_wage      0.203797585 -0.152809172 -0.141852712  0.21357476  0.22338514\n            hourly_wage     ln_wage\nwage         0.93124050  0.95314116\nhours       -0.31764547 -0.04721908\nIQ           0.26502635  0.31478770\nKWW          0.26059936  0.30627128\neduc         0.27167136  0.31211665\nexper        0.01775779  0.02060116\ntenure       0.13541822  0.18585262\nage          0.12668302  0.16182231\nmarried      0.11511570  0.14997589\nsouth       -0.14716118 -0.19481092\nurban        0.18924030  0.20379758\nsibs        -0.13136407 -0.15280917\nbrthord     -0.12029346 -0.14185271\nmeduc        0.18348733  0.21357476\nfeduc        0.20469678  0.22338514\nhourly_wage  1.00000000  0.89974921\nln_wage      0.89974921  1.00000000\n\n\nThe above table is informative but the reported numbers have far too many decimals. It is distracting our focus. Below, we round these in two decimal points, which is enough to have an idea about the strength of the correlation between our variables\n\n# Calculate pairwise correlations and store them under name cor_matrix\ncor_matrix &lt;- cor(wage2, use = \"pairwise.complete.obs\")\n\n# Round the correlation values to 2 decimal places and save them under the name rounded_cor_matrix\nrounded_cor_matrix &lt;- round(cor_matrix, 2)\n\n# Display the rounded correlation matrix\nprint(rounded_cor_matrix)\n\n             wage hours    IQ   KWW  educ exper tenure   age married south\nwage         1.00 -0.01  0.31  0.33  0.33  0.00   0.13  0.16    0.14 -0.16\nhours       -0.01  1.00  0.07  0.11  0.09 -0.06  -0.06  0.02    0.03 -0.03\nIQ           0.31  0.07  1.00  0.41  0.52 -0.22   0.04 -0.04   -0.01 -0.21\nKWW          0.33  0.11  0.41  1.00  0.39  0.02   0.14  0.39    0.09 -0.09\neduc         0.33  0.09  0.52  0.39  1.00 -0.46  -0.04 -0.01   -0.06 -0.10\nexper        0.00 -0.06 -0.22  0.02 -0.46  1.00   0.24  0.50    0.11  0.02\ntenure       0.13 -0.06  0.04  0.14 -0.04  0.24   1.00  0.27    0.07 -0.06\nage          0.16  0.02 -0.04  0.39 -0.01  0.50   0.27  1.00    0.11 -0.03\nmarried      0.14  0.03 -0.01  0.09 -0.06  0.11   0.07  0.11    1.00  0.02\nsouth       -0.16 -0.03 -0.21 -0.09 -0.10  0.02  -0.06 -0.03    0.02  1.00\nurban        0.20  0.02  0.04  0.10  0.07 -0.05  -0.04 -0.01   -0.04 -0.11\nsibs        -0.16 -0.05 -0.28 -0.28 -0.24  0.06  -0.04 -0.04    0.00  0.07\nbrthord     -0.15 -0.04 -0.18 -0.15 -0.20  0.09  -0.03  0.01   -0.01  0.09\nmeduc        0.21  0.08  0.33  0.24  0.36 -0.19  -0.01 -0.03   -0.02 -0.16\nfeduc        0.24  0.06  0.34  0.23  0.43 -0.26  -0.06 -0.07   -0.02 -0.17\nhourly_wage  0.93 -0.32  0.27  0.26  0.27  0.02   0.14  0.13    0.12 -0.15\nln_wage      0.95 -0.05  0.31  0.31  0.31  0.02   0.19  0.16    0.15 -0.19\n            urban  sibs brthord meduc feduc hourly_wage ln_wage\nwage         0.20 -0.16   -0.15  0.21  0.24        0.93    0.95\nhours        0.02 -0.05   -0.04  0.08  0.06       -0.32   -0.05\nIQ           0.04 -0.28   -0.18  0.33  0.34        0.27    0.31\nKWW          0.10 -0.28   -0.15  0.24  0.23        0.26    0.31\neduc         0.07 -0.24   -0.20  0.36  0.43        0.27    0.31\nexper       -0.05  0.06    0.09 -0.19 -0.26        0.02    0.02\ntenure      -0.04 -0.04   -0.03 -0.01 -0.06        0.14    0.19\nage         -0.01 -0.04    0.01 -0.03 -0.07        0.13    0.16\nmarried     -0.04  0.00   -0.01 -0.02 -0.02        0.12    0.15\nsouth       -0.11  0.07    0.09 -0.16 -0.17       -0.15   -0.19\nurban        1.00 -0.03    0.00  0.03  0.11        0.19    0.20\nsibs        -0.03  1.00    0.59 -0.29 -0.23       -0.13   -0.15\nbrthord      0.00  0.59    1.00 -0.28 -0.23       -0.12   -0.14\nmeduc        0.03 -0.29   -0.28  1.00  0.58        0.18    0.21\nfeduc        0.11 -0.23   -0.23  0.58  1.00        0.20    0.22\nhourly_wage  0.19 -0.13   -0.12  0.18  0.20        1.00    0.90\nln_wage      0.20 -0.15   -0.14  0.21  0.22        0.90    1.00\n\n\n\n\n\n5.2.6 Task 6\n\n5.2.6.1 Task\nExamine the relationship between education and wage using a scatter plot.\n\n\n5.2.6.2 Guidance\nWe use the ggplot2 package to draw plots. First install the package and call the library.\n\n# install.packages(\"ggplot2\")\n library(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:expss':\n\n    vars\n\n\nEducation is expected to have a positive impact on wage. In our scatter plot, educ will be on the horizontal-axis while wage will be on the vertical-axis.\n\n# Scatter plot\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  labs(title = \"Scatter plot of Wage vs. Education\", x = \"Years of Schooling\", y = \"Wage\")\n\n\n\n\n\n\n\n\nYou see above the full set of lines to create this plot. But let us do this step by step to have a better understanding. First, we bring the educ and wage variables from the wage2 data and position these on our plot.\n\nggplot(wage2, aes(x = educ, y = wage))\n\n\n\n\n\n\n\n\nWe then add (using the + sign), the observations in our data, represented by dots.\n\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() \n\n\n\n\n\n\n\n\nIt is always good practice to give a title for your plot. Notice also that the horizontal and vertical axes above are labelled by the variable names. We may also replace these with proper definitions of the variables. This is to make it easier for the readers to understand your plots:\n\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  labs(title = \"Scatter plot of Wage vs. Education\", x = \"Years of Schooling\", y = \"Wage\")\n\n\n\n\n\n\n\n\n\n\n\n5.2.7 Task 7\n\n5.2.7.1 Task\nTabulate the urban variable to see the distribution of observations in rural and urban areas\n\n\n5.2.7.2 Guidance\nWe use the table() function for that purpose.\n\ntable(wage2$urban)\n\n\n  0   1 \n264 671 \n\n\n\n\n\n5.2.8 Task 8\n\n5.2.8.1 Task\nLet’s say we are interested to plot the education-wage relationship differentiating between people in rural and urban areas. Replicate the scatter plot above, but this time, using different colors for rural and urban.\n\n\n5.2.8.2 Guidance\nNotice how we add the color = urban option below. We do the same for the label too.\n\n# Scatter plot - colored by urban\nggplot(wage2, aes(x = educ, y = wage, color = urban)) +\n  geom_point() +\n  labs(title = \"Scatter plot of Wage vs. Education\", x = \"Years of Schooling\", y = \"Wage\", color = \"urban\")\n\n\n\n\n\n\n\n\nThe labelling of the above plot looks as if we have a range of values for the urban variable, changing from zero to one. The urban variable, in fact, is a dummy, taking two values only: zero for rural and one for urban residence. If you look into this variable entry in more detail, you will see that it is stored as num. We can change this using the factor() function. Instead of overriding the urban variable, let’s create a new variable urban_residence to see a comparison.\n\nwage2$urban_residence &lt;- factor(wage2$urban, levels = c(0,1), labels = c(\"rural\", \"urban\"))\n\nBelow, we view the two variables using R’s dplyr package.\n\n# install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:expss':\n\n    compute, contains, na_if, recode, vars, where\n\n\nThe following objects are masked from 'package:maditr':\n\n    between, coalesce, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#View(select(wage2, urban, urban_residence))\n\nLet’s re-run our scatter plot code again (but replacing urban with urban_residence:\n\n# Scatter plot - colored by urban\nggplot(wage2, aes(x = educ, y = wage, color = urban_residence)) +\n  geom_point() +\n  labs(title = \"Scatter plot of Wage vs. Education\", x = \"Years of Schooling\", y = \"Wage\", color = \"urban_residence\")\n\n\n\n\n\n\n\n\n\n\n\n5.2.9 Task 9\n\n5.2.9.1 Task\nEstimate a regression model where wage is regressed on education. Interpret the results.\n\n\n5.2.9.2 Guidance\nWe use the lm() function to estimate linear regression models. You may read ~ in wage ~ educ below as “approximately modelled as” James et al. (2023). We may also say “wage is regressed on education”.\n\n# Linear regression\nmodel_1 &lt;- lm(wage ~ educ, data = wage2)\nsummary(model_1)\n\n\nCall:\nlm(formula = wage ~ educ, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-877.38 -268.63  -38.38  207.05 2148.26 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  146.952     77.715   1.891   0.0589 .  \neduc          60.214      5.695  10.573   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 382.3 on 933 degrees of freedom\nMultiple R-squared:  0.107, Adjusted R-squared:  0.106 \nF-statistic: 111.8 on 1 and 933 DF,  p-value: &lt; 2.2e-16\n\n\nIn the above regression output, we see that education has a statistically significant impact on wages. Each year of schooling increases wage by around £60, on average. The F test tells us that the regression model has an explanatory power, even though the R-squared value is low.\n\n\n\n5.2.10 Task 10\n\n5.2.10.1 Task\nUsing the regression model above, predict what the wage would be for given values of education (how much do we expect the wage would be for given years of schooling).\n\n\n5.2.10.2 Guidance\nBelow, we recall model_1 to calculate predicted values; save the predictions under name wage_hat under wage2 data.\n\n# Save predicted values under name wage_hat\nwage2$wage_hat &lt;- predict(model_1)\n\n\n\n\n5.2.11 Task 11\n\n5.2.11.1 Task\nAdd the estimated regression line to the wage-education scatter plot.\n\n\n5.2.11.2 Guidance\nWe will be adding the regression line to the scatter plot we produced above. We use geom_smooth for this purpose. Let’s first remember what we did before:\n\n# Scatter plot of education and wage\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\n\n\n\n\n\n\n\n\nNow, let’s add the regression line:\n\n# Scatter plot with fitted line\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nthe geom_smooth(method = \"lm\") asks R to add a line estimating a “linear model” (i.e. a regression) of wage on educ.\nNote that we could save this plot as an object by assigning it a name on the left hand side of the command. We will do that below and name the plot as scatter_wage_educ.\nCan you guess what the plot would look if we changed se = FALSE to se = TRUE above? We can also try that below:\n\n# Scatter plot with fitted line\nscatter_wage_educ &lt;- ggplot(wage2, aes(x = educ, y = wage)) +\n                        geom_point() +\n                        geom_smooth(method = \"lm\", se = TRUE) +\n                        labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\nprint(scatter_wage_educ)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe could also add this sample regression line by using the wage_hat variable. wage_hat shows the predicted value of wage given observed values of education.\n\n# Scatter plot with fitted line\n# we add the wage_hat variable\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  geom_line(aes(y = wage_hat), color = \"blue\", size = 1) +\n  labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nNote that we used geom_line() this time to add a line plot of an already existing variable in the data set.\n\nggplot(wage2, aes(x = educ, y = wage)) creates a canvas, a plot area with educ at the horizontal and wage at the vertical axis\ngeom_point() adds a scatterplot of wage against educ.\ngeom_line(aes(y = wage_hat)) adds the line for the predicted wage_hat values. The aes(y = wage_hat) ensures the line graph uses wage_hat on the y-axis while sharing the x-axis (educ).\ncolor and size are optional for styling the line. Try experimenting with these and observe the changes.\n\n\n\n\n5.2.12 Task 12\n\n5.2.12.1 Task\nEstimate a multiple regression model by adding experience and urban residence into the above regression. Save it under name model_2\n\n\n5.2.12.2 Guidance\nWe will add exper and urban variables into the regression model using + sign.\n\n# Linear regression\nmodel_2 &lt;- lm(wage ~ educ + exper + urban, data = wage2)\nsummary(model_2)\n\n\nCall:\nlm(formula = wage ~ educ + exper + urban, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-799.67 -234.04  -34.26  197.89 2119.62 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -362.821    106.419  -3.409 0.000679 ***\neduc          74.119      6.193  11.968  &lt; 2e-16 ***\nexper         17.940      3.105   5.777 1.03e-08 ***\nurban        160.306     26.920   5.955 3.69e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 369.5 on 931 degrees of freedom\nMultiple R-squared:  0.1676,    Adjusted R-squared:  0.1649 \nF-statistic: 62.47 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nHow does model_2 compare to model_1?\n\n\n\n5.2.13 Task 13\n\n5.2.13.1 Task\nSave your data to keep the newly created hourly_wage and ln_wage variables.\n\n\n5.2.13.2 Guidance\n\n# Save data in R format\nsave(wage2, file = \"./assets/data/wage2.Rdata\")\n\n\n\n\n5.2.14 A Gentle Introduction to dplyr library\nThe dplyr library comes with R’s tidyverse package. The ggplot2 library we used above to produce plots is also a part of the tidyverse package.\nI will replicate below a few of the tasks that we performed above using the dplyr library\n\n5.2.14.1 Viewing data\nWe have seen before to use View to see the contents of data in a spreadsheet format:\n\nhead(wage2)\n\n# A tibble: 6 × 19\n   wage hours    IQ   KWW educ      exper tenure   age married south urban  sibs\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;labelle&gt; &lt;lab&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   769    40    93    35 12        11         2    31       1     0     1     1\n2   808    50   119    41 18        11        16    37       1     0     1     1\n3   825    40   108    46 14        11         9    33       1     0     1     1\n4   650    40    96    32 12        13         7    32       1     0     1     4\n5   562    40    74    27 11        14         5    34       1     0     1    10\n6  1400    40   116    43 16        14         2    35       1     0     1     1\n# ℹ 7 more variables: brthord &lt;dbl&gt;, meduc &lt;dbl&gt;, feduc &lt;dbl&gt;,\n#   hourly_wage &lt;dbl&gt;, ln_wage &lt;dbl&gt;, urban_residence &lt;fct&gt;, wage_hat &lt;dbl&gt;\n\n#View(wage2)\n\nWe may use dplyr to select variables for viewing. Using select allows us to “keep or drop columns using their names and types”.\n\n#View(select(wage2, wage, educ, exper, urban, urban_residence))\n\n\n\n5.2.14.2 Generating new variables\nWe used the following lines to create hourly_wage and ln_wage variables:\n\n# Generate new variables\nwage2$hourly_wage &lt;- wage2$wage / wage2$hours\nwage2$ln_wage &lt;- log(wage2$wage)\n\ndplyr ’s mutate us used to “create, modify, and delete columns”. Let us create a new data frame, wage2_new to see what it does:\n\nwage2_new &lt;- wage2 %&gt;% \n  mutate (\n    hourly_wage_n = wage / hours,\n    ln_wage_n = log(wage)\n  )\n\nIn the above lines, we create a new data frame based on wage2 . Note the %&gt;% above. This is a part of the command and is called the pipe operator. It helps us to simply the code and do the operations one step after another. We first call wage2 and create the new variables, hourly_wage_n and ln_wage_n .\nNote how we avoided the use of wage2$ every time we referred to a variable in wage2 data.\nAnother example we used to create a new variable was when we predicted values of wage for given levels of education after estimating model_1.\nBelow is the code we used:\n\nwage2$wage_hat &lt;- predict(model_1)\n\nWe can do this as follows using dplyr\n\nwage2 &lt;- wage2 %&gt;% \n  mutate(\n    wage_hat_n = predict(model_1)\n  )\n\n\n\n5.2.14.3 Tabulating Variables\nWe used the code below to tabulate values of urban variable\n\ntable(wage2$urban)\n\n\n  0   1 \n264 671 \n\n\nwe may use count in dplyr for this purpose\n\nwage2 %&gt;% \n  count(urban)\n\n# A tibble: 2 × 2\n  urban     n\n  &lt;dbl&gt; &lt;int&gt;\n1     0   264\n2     1   671\n\n\nRemember that we could save this as a new object:\n\nurban_table &lt;- wage2 %&gt;% \n  count(urban)\nprint(urban_table)\n\n# A tibble: 2 × 2\n  urban     n\n  &lt;dbl&gt; &lt;int&gt;\n1     0   264\n2     1   671\n\n\nWhich output do you prefer?",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Regression Analysis</span>"
    ]
  },
  {
    "objectID": "introduction-to-regression-analysis.html#further-exercises",
    "href": "introduction-to-regression-analysis.html#further-exercises",
    "title": "5  Introduction to Regression Analysis",
    "section": "5.3 Further Exercises",
    "text": "5.3 Further Exercises\nDownload the data set called EAWE21.Rdata from the module page on Aula and save it. This is a subset of the Educational Attainment and Wage Equations data set used in Dougherty (2016) available from https://global.oup.com/uk/orc/busecon/economics/dougherty5e/student/datasets/eawe/. For this exercise we are interested in two variables:\n\nEXP : Total out-of-school work experience (years) as of the 2002 interview\nEARNINGS : Current hourly earnings in $ reported at the 2002 interview\n\n\n5.3.1 Tasks\n\nCalculate summary statistics (mean, median, minimum, maximum) for the variables EXP and EARNINGS\nDraw scatter plot of EARNINGS on EXP.\nCalculate the covariance and correlation between earnings and exp and comment on the values\nRegress EARNINGS on EXP and comment on\n\nthe sign and size of the regression coefficients\nthe goodness of fits of the estimated model.\n\nAdd a regression line to the scatter plot.\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2023. An Introduction to Statistical Learning. 2nd edition. Springer. https://www.statlearning.com.\n\n\nRiegler, Robert. 2022. “R Workbook - Guidance for Worksheets.” Aston University.",
    "crumbs": [
      "Seminar 2 (28 January 2025)",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Regression Analysis</span>"
    ]
  },
  {
    "objectID": "multiple-regression-and-diagnostic-checks.html",
    "href": "multiple-regression-and-diagnostic-checks.html",
    "title": "6  Multiple Regression and Diagnostic Checks",
    "section": "",
    "text": "6.1 Example: wage data\nWe will use the wage2 data set, which is already saved in Rdata format.",
    "crumbs": [
      "Seminar 3 (4 February 2025)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Regression and Diagnostic Checks</span>"
    ]
  },
  {
    "objectID": "multiple-regression-and-diagnostic-checks.html#example-wage-data",
    "href": "multiple-regression-and-diagnostic-checks.html#example-wage-data",
    "title": "6  Multiple Regression and Diagnostic Checks",
    "section": "",
    "text": "6.1.1 Task 1\nOpen wage2.Rdata (if it is not already open). You may so this through the menu or the command line using the load() function:\n\nload(\"~/Desktop/R-workshops/assets/data/wage2.Rdata\")\n\n\n\n6.1.2 Task 2\nEstimate a multiple regression model by regressing wage on IQ, educ, exper, urban and save it under name model_3 . Display the estimation results.\n\n6.1.2.1 Guidance\n\n# Linear regression\nmodel_3 &lt;- lm(wage ~ IQ + educ + exper + urban, data = wage2)\nsummary(model_3)\n\n\nCall:\nlm(formula = wage ~ IQ + educ + exper + urban, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-797.64 -229.84  -38.35  185.10 2082.22 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -628.8654   115.5135  -5.444 6.66e-08 ***\nIQ             5.0564     0.9234   5.476 5.60e-08 ***\neduc          56.0554     6.9340   8.084 1.94e-15 ***\nexper         17.7194     3.0583   5.794 9.41e-09 ***\nurban        159.9813    26.5107   6.035 2.30e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 363.9 on 930 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1901 \nF-statistic: 55.81 on 4 and 930 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n6.1.3 Task 3\n\n6.1.3.1 Task\nTest for the normality of the residuals\n\n\n6.1.3.2 Guidance\nWe will be using the Jarque-Bera test for this purpose.\nWe first save the residuals from model_3 .\n\nwage2$resid_m3 &lt;- residuals(model_3)\n\nPlot the residuals to see the distribution. Please note that is not a part of the test but visualisation helps us to understand the data better.\n\nlibrary(ggplot2)\nggplot(wage2, aes(x = resid_m3)) +\n  geom_histogram(binwidth = 200, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram of Residuals (model_3)\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\naes(x = resid) specifies the residuals as the variable for the x-axis.\ngeom_histogram() is used to create the histogram:\n\nbinwidth = 200 controls the width of the bins. You can adjust this depending on how detailed you want the histogram to be.\nfill sets the color inside the bars, and color adds a border around them for better visibility.\n\nlabs() adds labels for the title and axes.\ntheme_minimal() gives a clean, simple look to the plot - try the plot with and without this.\n\nYou may also let ggplot choose the number of bins automatically:\n\nggplot(wage2, aes(x = resid_m3)) +\n  geom_histogram(fill = \"pink\", color = \"black\") +\n  labs(title = \"Histogram of Residuals (model_3)\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe may use jarque.bera.test for the normality test. It is in the tseries package.\n\n# install.packages(\"tseries\")\nlibrary(tseries)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\njarque.bera.test(wage2$resid_m3)\n\n\n    Jarque Bera Test\n\ndata:  wage2$resid_m3\nX-squared = 699.59, df = 2, p-value &lt; 2.2e-16\n\n\nThe p-value of the test is almost zero. We reject the null hypothesis of normal distribution. The residuals from model_3 are not normally distributed.\n\n\n\n6.1.4 Task 4\n\n6.1.4.1 Task\nTest for the functional form.\n\n\n6.1.4.2 Guidance\nWe may use this to check whether there are any omitted variables or non-linearity in the model. The test is Ramsey RESET.\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nresettest(model_3)\n\n\n    RESET test\n\ndata:  model_3\nRESET = 3.8665, df1 = 2, df2 = 928, p-value = 0.02127\n\n\nThe default resettest includes second and third powers of the fitted values in the test regression. You may change this using the power option. Below we include from second to the fourth power of fitted values.\n\nresettest(model_3, power = 2:4)\n\n\n    RESET test\n\ndata:  model_3\nRESET = 2.8504, df1 = 3, df2 = 927, p-value = 0.03646\n\n\nThe decision depends on the chosen significance level. We reject the null hypothesis of correct functional form if we choose a 5% significance level.\n\n\n\n6.1.5 Task 5\n\n6.1.5.1 Task\nTest for heteroscedasticity.\n\n\n6.1.5.2 Guidance\nWe apply Breusch-Pagan heteroscedasticity test.\n\nbptest(model_3)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_3\nBP = 16.355, df = 4, p-value = 0.002578\n\n\nThe p-value is smaller than 0.05. Hence, we reject the null of no heteroscedasticity at 5% significance level. There is heteroscedasticity.\n\n\n\n6.1.6 Task 6\n\n6.1.6.1 Task\nTest for autocorrelation in the model\n\n\n6.1.6.2 Guidance\nThis is a trick question! Autocorrelation problem is related to time series data whereas we have cross-section data here. Autocorrelation problem is irrelevant here.\n\n\n\n6.1.7 Task 7\n\n6.1.7.1 Task\nReplicate the above using logarithmic wages. Has there been a change in model diagnostics? Which form do you prefer to use for inference?\n\n\n6.1.7.2 Guidance\nYou may use the script file to copy-paste all the code and make the minor changes (i.e. replacement of wage with ln_wage).",
    "crumbs": [
      "Seminar 3 (4 February 2025)",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Multiple Regression and Diagnostic Checks</span>"
    ]
  },
  {
    "objectID": "introduction-to-time-series-analysis.html",
    "href": "introduction-to-time-series-analysis.html",
    "title": "7  Introduction to Time Series Analysis",
    "section": "",
    "text": "7.1 Example: GAP Sales data\nWe start by loading the required libraries.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(lmtest)\n\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(tseries)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo\nThe GAP_Sales data that we will be using in this session is obtained from (Wilson and Keating 2007). It shows the sales figures of GAP.",
    "crumbs": [
      "Seminar 4 (11 February 2025)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "introduction-to-time-series-analysis.html#example-gap-sales-data",
    "href": "introduction-to-time-series-analysis.html#example-gap-sales-data",
    "title": "7  Introduction to Time Series Analysis",
    "section": "",
    "text": "7.1.1 Task 1\nStart a new project in R and name it as GAP-sales-analysis. Import GAP_Sales.csv data into this project. GAP_Sales is a quarterly time series data covering time period 1985:Q1 to 2004:Q4. In this example, we would like to estimate a regression model explaining sales of GAP.\n\n7.1.1.1 Guidance\nUse the menu import GAP_Sales.csv file into R. You need to choose From Text (base) because csv is a text format. The GAP_Sales data we have is comma separated, but you may encounter a different form of separation, for example, tab or semi-column. In the opening window, give a name for your data frame under the Name field and remember to check the Heading as Yes because we have variable names in the first row of the csv file. Also, note the strings as factors option, which asks R to import text-based content (variables) as categorial (factor is the terminology R uses).\nYou could alternatively run the code below\n\ndf &lt;- read.csv(\"~/Desktop/R-workshops/assets/data/GAP_Sales.csv\", stringsAsFactors=TRUE)\n#View(df)\n\nFor ease of typing, I called this data as df. In the code below, df will refer to the GAP_Sales data we imported.\n\n\n\n7.1.2 Task 2\nBrowse the data and see the contents of the variables.\n\n7.1.2.1 Guidance\nWe have done this above, using\n\n#View(df)\n\nYou may also use head() function to see the first 6 rows of data\n\nhead(df)\n\n  Year quarter   Yqrt  Sales Time T.squared Q2 Q3 Q4 D911      ICS\n1 1985      q1 1985q1 105715    1         1  0  0  0    0 94.46667\n2 1985      q2 1985q2 120136    2         4  1  0  0    0 94.30000\n3 1985      q3 1985q3 181669    3         9  0  1  0    0 92.83333\n4 1985      q4 1985q4 239813    4        16  0  0  1    0 91.06667\n5 1986      q1 1986q1 159980    5        25  0  0  0    0 95.53333\n6 1986      q2 1986q2 164760    6        36  1  0  0    0 96.76667\n\n\n\n\n\n7.1.3 Task 3\nProvide a time series plot of the Sales variable.\n\n7.1.3.1 Guidance\nGAP_Sales data is a quarterly data. However, R would not recognise this until we tell it that is a quarterly time series. R has a built-in time series class, ts for basic data manipulation. Some other popular packages (more advanced than the ts in base R) include tseries and zoo.\nAs (Kleiber and Zeileis 2008) explains, ts is aimed at regular series observed in annual, quarterly, and monthly intervals. Time series objects can be created by supplying the data along with the arguments start, end, and frequency. The data can be:\n\na numeric vector (a single variable), or\na matrix (including a set of variables).\n\nIt includes time-series specific methods such as lag() (for the lagged values of the variables) and diff() (for time differencing the variable).\nSales is the variable we are interested in our data. So, let us start by introducing a time dimension to that series. In the code below, we create a single numeric vector, gap_sales_ts by defining the start date and the frequency of the Sales variable. Our variable starts from the first quarter of 1985 with a frequency of 4 (it is a quarterly data, repeating every 3 months).\n\ngap_sales_ts &lt;- ts(df$Sales, start = c(1985, 1), frequency =4)\n\nR’s basic plot function will give us the following:\n\nplot(gap_sales_ts)\n\n\n\n\n\n\n\n\nYou may add labels and color with some additional options:\n\nplot(gap_sales_ts, col = \"blue\", lwd = 2, xlab = \"Year\", ylab = \"Sales\", main = \"Quarterly Sales of GAP\")\n\n\n\n\n\n\n\n\nYou may also use ggplot to plot the Sales data:\n\nggplot(df, aes(x = Time, y = Sales)) +\n  geom_line(color = \"blue\", size = 1) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nIn the above plot, although we can see the pattern of the Sales variable quite clearly, the Time variable labels fail to show us the respective quarter values. We may change these labels by using the following lines of code.\nWe first define labels to correspond to each data point\n\n# First, create a new column for formatted quarter labels\ndf$Quarter_label &lt;- paste0(df$Year, \":\", df$quarter)\n# You can achieve the same as above using the code below: \n# (note that you do not need this once you create Quarter_Label above )\ndf$Quarter_label_v2 &lt;- with(df, paste(Year, quarter, sep = \":\"))\n\nCheck the values of Quarter_label in the df. You will see that it goes on like 1985:q1, 1985:q2, and so on. We may now use these labels instead of the values of the Time variable.\n\nggplot(df, aes(x = Time, y = Sales)) +\n  geom_line(color = \"blue\", size = 1) +\n  scale_x_continuous(\n    breaks = df$Time,  # Position the breaks at each quarter, i.e. at each value of Time\n    labels = df$Quarter_label  # Label each point using Quarter_label variable created above\n  ) + # provide a title and axes labels below\n  labs(title = \"Quarterly GAP Sales\", x = \"Quarter\", y = \"Sales\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, size=6))  # Rotate labels for better readability using the angle option and set the font size for label using size option\n\n\n\n\n\n\n\n\nLooking at this plot, what can you say about the sales figures over time? What kind of time-series characteristics it reveals?\n\nWe can see above that the gap sales have some repeating fluctuations around a positive trend. The trend is not linear. The regression that we estimate should capture this non-linear trend as well as the seasonal fluctuations.\nThe following tasks will take us to the best fit possible with the available data.\n\n\n\n\n7.1.4 Task 4\nFit a linear trend line to the Sales variable.\na. Provide an interpretation of the slope coefficient.\nb. Check how well this model fits the data by plotting the predictions of the model and the observed values against time.\nc. Plot the residuals of this model and explain whether or not you see a pattern.\n\n7.1.4.1 Guidance\nThe Time variable will be used to fit a linear trend to Sales. The Time variables takes values from 1 to 80, increasing by 1 in each data point (quarter).\n\n# Fit a linear trend line to Sales data\nmodel_1 &lt;- lm(Sales ~ Time, data = df)\nsummary(model_1)\n\n\nCall:\nlm(formula = Sales ~ Time, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-889709 -390551  -60886  325202 1600763 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -680044     121435   -5.60 3.08e-07 ***\nTime           57162       2605   21.95  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 538000 on 78 degrees of freedom\nMultiple R-squared:  0.8606,    Adjusted R-squared:  0.8588 \nF-statistic: 481.6 on 1 and 78 DF,  p-value: &lt; 2.2e-16\n\n\n\nTime variable takes values starting from 1 and increasing by 1 in each quarter. Including this variable will allow us to fit a trend to sales.\nInterpretation of the coefficient of Time variable: In each quarter, the GAP sales increases by $57,162 thousand, on average (note that in the data, Sales is measured in thousand dollars)\n\nObtain predictions using predict() function.\n\n# Obtain predictions\ndf$sales_hat_m1 &lt;- predict(model_1)\n\nWe can use R’s base time series plot but we will need to convert the predictions into a time series. Alternatively, ggplot is easier to use.\n\n# Plot actual versus predicted Sales\nggplot(df, aes(x = Time)) +\n  geom_line(aes(y = Sales, color = \"Actual\")) +\n  geom_line(aes(y= sales_hat_m1, color = \"Predicted\")) +\n  theme_minimal() +\n  labs(title = \"Actual versus Predicted Sales\", x = \"Time\", y = \"Sales\")\n\n\n\n\n\n\n\n\n\nWe can see above that although we could estimate the trend roughly, it is not a perfect fit. Sales has a positive trend, it is not linear. We will be using natural logarithm of sales below.\n\nBelow, we save and plot the residuals from model_1\n\n# Save residuals from model_1\ndf$residuals_m1 &lt;- residuals(model_1)\n\n# Residual plot\nggplot(df, aes(x = Time, y = residuals_m1)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Residuals from Model 1\", x = \"Time\", y = \"Residuals\")\n\n\n\n\n\n\n\n\n\nWhen we look at the plot of residuals, we can see that the deviations from the linear trend (i.e. the non-linearities and the fluctuations) are reflected in residuals.\n\n\n\n\n7.1.5 Task 5\nReplicate the same analysis using logarithm of Sales\n\n7.1.5.1 Guidance\nWe start by taking the logarithm of Sales variable.\n\n# Logarithmic Sales data\ndf$ln_sales &lt;- log(df$Sales)\n\nPlot logarithmic sales. Let’s first do this base R’s time series plot. We start by converting our ln_sales into quarterly time series, and then use the plot() function. lwd option below sets the line width of the plot. Change the color and the lwd values and see what you get.\n\n# Plot of logarithmic sales (first approach - convert to time series)\nln_sales_ts &lt;- ts(df$ln_sales, start = c(1985, 1), frequency =4) # covenrt the ln_sales into time series\nplot(ln_sales_ts, col = \"purple\", lwd = 2, xlab = \"Year\", ylab = \"Logarithmic Sales\", main = \"Quarterly LogarithmicSales of GAP\")\n\n\n\n\n\n\n\n\nWe may also use ggplot for the same purpose\n\n# Plot of logarithmic sales (second approach - use ggplot)\nggplot(df, aes(x = Time, y = ln_sales)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Logaritmic Sales\", x = \"Time\", y = \"Logarithmic sales\")\n\n\n\n\n\n\n\n\nFit a trend line to logarithmic sales\n\n# Fit a trend line to logarithmic sales\nmodel_2 &lt;- lm(ln_sales ~ Time, data = df)\nsummary(model_2)\n\n\nCall:\nlm(formula = ln_sales ~ Time, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4883 -0.1559 -0.0026  0.1684  0.4589 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.011834   0.046964  255.76   &lt;2e-16 ***\nTime         0.044919   0.001007   44.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2081 on 78 degrees of freedom\nMultiple R-squared:  0.9623,    Adjusted R-squared:  0.9618 \nF-statistic:  1988 on 1 and 78 DF,  p-value: &lt; 2.2e-16\n\n\n\nBoth the intercept and slope coefficients are statistically significant (with very low p-values). Slope coefficient this time shows that in each quarter, sales increase by around 4.5%, on average.\n\nLet’s now plot the predictions from this model with the actual ln_sales figures\n\n# Obtain predictions from the logarithmic model\ndf$ln_sales_hat_m2 &lt;- predict(model_2)\n\n# Plot actual versus predicted log sales\nggplot(df, aes(x = Time)) +\n  geom_line(aes(y = ln_sales, color = \"Actual\")) +\n  geom_line(aes(y = ln_sales_hat_m2, color = \"Predicted\")) +\n  theme_minimal() +\n  labs(title = \"Actual versus predicted logarithmic sales\", x = \"Time\", y = \"logarithmic sales\")\n\n\n\n\n\n\n\n\nWhat do you think about this fit?\n\nWe can see above that using a logarithmic transformation helped to obtain a better fit for sales.\nNote that although it is very tempting to use \\(R^2\\) to compare the goodness-of-fit of these two models (with and without the logarithmic transformation), we cannot do that as \\(R^2\\) cannot be used to compare models with different dependent variables.\n\nLet’s check what the residuals from the above estimation look like\n\n# Residuals from model_2\ndf$residuals_m2 &lt;- residuals(model_2)\n\n# Plot residuals from model_2\nggplot(df, aes(x = Time, y= residuals_m2))+\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Residuals from model 2\", x = \"Time\", y = \"Residuals\")\n\n\n\n\n\n\n\n\n\nWe can see that the residuals repeatedly fluctuate in certain intervals. This is due to the seasonality in the sales data. We will be using quarter dummies to control for the seasonality.\nIn comparison to the previous regression specification, residuals do not reveal a trend (because by fitting a trend on the logarithmic data, we have controlled for the non-linear trend.\n\n\n\n\n7.1.6 Task 6\nDo you have any suggestions to improve the fit of this model?\n\n7.1.6.1 Guidance\nCheck the residual plot above. Do you see a specific pattern? What can we do to capture the fluctuations that you see?\n\n\n\n7.1.7 Task 7\nAdd quarter dummies to the model you estimated above.\na. Interpret the coefficients in this model.\nb. Check how well this model fits the data by plotting the predictions of the model and the observed values against time.\nc. Plot the residuals of this model and explain whether or not you see a pattern.\nd. Does the inclusion of the quarter dummies improve the fit of the model? Test for the joint significance of the quarter dummies.\ne. If you were to choose one the models that you have estimated using the GAP sales data, which one would you choose? Why?\n\n7.1.7.1 Guidance\nThe quarter dummies that we need for this model are already in the data: Q2, Q3, Q4. If these were not in the data, we could create them using the ifelse() function. This is provided below\n\ndf$qrt_1 &lt;- ifelse(df$quarter == \"q1\", 1, 0)\ndf$qrt_2 &lt;- ifelse(df$quarter == \"q2\", 1, 0)\ndf$qrt_3 &lt;- ifelse(df$quarter == \"q3\", 1, 0)\ndf$qrt_4 &lt;- ifelse(df$quarter == \"q4\", 1, 0)\n\nWe could also use the dplyr package (note that I assign different names to these variables to be able to differentiate alternative ways of creating the dummies. You may choose a name of your own):\n\ndf &lt;- df %&gt;%\n  mutate(\n    quarter1 = ifelse(quarter == \"q1\", 1, 0),\n    quarter2 = ifelse(quarter == \"q2\", 1, 0),\n    quarter3 = ifelse(quarter == \"q3\", 1, 0),\n    quarter4 = ifelse(quarter == \"q4\", 1, 0)\n  )\n\nCheck the values of these newly created dummies (quarter1, quarter2, quarter3, and quarter4) in the data.\nWe can now estimate the model including these quarter dummies together with a linear trend\n\n# Estimate the model using trend and quarter dummies\nmodel_3 &lt;- lm(ln_sales ~ Time + quarter2 + quarter3 + quarter4, data = df)\nsummary(model_3)\n\n\nCall:\nlm(formula = ln_sales ~ Time + quarter2 + quarter3 + quarter4, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41512 -0.06014 -0.00347  0.09422  0.23885 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.882418   0.042709 278.219  &lt; 2e-16 ***\nTime         0.044620   0.000707  63.110  &lt; 2e-16 ***\nquarter2     0.013792   0.046130   0.299 0.765783    \nquarter3     0.184808   0.046146   4.005 0.000145 ***\nquarter4     0.367414   0.046173   7.957 1.44e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1459 on 75 degrees of freedom\nMultiple R-squared:  0.9822,    Adjusted R-squared:  0.9812 \nF-statistic:  1032 on 4 and 75 DF,  p-value: &lt; 2.2e-16\n\n\n\nThere are 4 quarters in a year. Quarter 1 is excluded from the model. This is the reference category. Our interpretation of the other quarter dummies will be in reference to the excluded category. Looking at the p-values, Q3 and Q4 are statistically significant while Q2 is statistically insignificant. Statistical insignificance implies that sales in quarter 2 are not different than sales in quarter 1. In other words, average sales in the first 2 quarters are around the same level.\nA rough interpretation of the Q3 and Q4 coefficients would be as follows (note that this approach could be reliable only when the effects (i.e. the coefficients) are very small)\n\nHolding everything else constant, sales in quarter 3 (i.e. in months July-August- September) are around 18.5% higher than sales in quarter 1 (i.e. in comparision to sales in the first 3 months of the year.\nHolding everything else constant, sales in quarter 4 (i.e. in months October- November-December) are around 36.7% higher than sales in quarter 1 (i.e. in comparison to sales in the first 3 months of the year.\n\nFor a more precise interpretation of dummy variable coefficients in a logarithmic dependent variable model, we need to transform the estimated coefficients first:\n\\[\n[exp(0.1848)-1] \\times 100 = 20.30\n\\]\n\\[\n[exp(0.3674)-1] \\times 100 = 44.40\n\\]\n\nHolding everything else constant, sales in quarter 3 (i.e. in months July-August- September) are around 20.30% higher than sales in quarter 1 (i.e. in comparison to sales in the first 3 months of the year\nHolding everything else constant, sales in quarter 4 (i.e. in months October- November-December) are around 44.40% higher than sales in quarter 1 (i.e. in comparison to sales in the first 3 months of the year.\n\nPlease note that this transformation is applied only when the dependent variable is in logarithmic form and if we are commenting on the effect of a dummy variable.\n\nAre these quarterly dummies contributing to the explanatory power of the model? In other words, are they jointly statistically significant? We can check this using an F-test for restrictions. This could be done using the anova function in R.\nBelow are the steps we follow to test for the restrictions:\n\nEstimate the full (unrestricted) model. We have done that above. It is saved under model_3.\nEstimate the restricted model where quarter dummy coefficients take value zero. This is in fact, our model_2 above.\nPerform an F-test to compare the restricted and unrestricted models using anova() :\nanova(restricted_model, unrestricted_model)\n\n\n# Perform an F-test\nanova(model_2, model_3)\n\nAnalysis of Variance Table\n\nModel 1: ln_sales ~ Time\nModel 2: ln_sales ~ Time + quarter2 + quarter3 + quarter4\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    \n1     78 3.3767                                 \n2     75 1.5956  3    1.7811 27.906 3.17e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe null hypothesis in the above test is that the coefficients of quarter dummies are jointly equal to zero versus the alternative that at least one is different than zero. We have a very small p-value. Hence we reject the null hypothesis and conclude that the quarter dummies are jointly statistically significant.\n\nLet’s plot the actual values against predictions to see the improvement by the inclusion of the quarter\n\n# Obtain predictions from model_3\ndf$ln_sales_hat_m3 &lt;- predict(model_3)\n\n# Plot actual versus predicted log sales\nggplot(df, aes(x = Time)) +\n  geom_line(aes(y = ln_sales, color = \"Actual\")) +\n  geom_line(aes(y = ln_sales_hat_m3, color = \"Predicted\")) +\n  theme_minimal() +\n  labs(title = \"Actual versus predicted logarithmic sales\", x = \"Time\", y = \"logarithmic sales\")\n\n\n\n\n\n\n\n\n\n\nWe can see from the plot that the regression model now have a better fit to the actual data. All fluctuations are captured by quarter dummies.\n\n\nAnd finally, let’s have a look at the residuals\n\n# Residuals from model_2\ndf$residuals_m3 &lt;- residuals(model_3)\n\n# Plot residuals from model_2\nggplot(df, aes(x = Time, y= residuals_m3))+\n  geom_line(color = \"purple\") +\n  theme_minimal() +\n  labs(title = \"Residuals from model 3\", x = \"Time\", y = \"Residuals\")\n\n\n\n\n\n\n\n\n\nAmong the 3 models estimated, we would choose the last one because it has a better fit than the others. In addition to the checks we have done above, you could also compare the residual sum of squares to see which one fits better. But please note that if you follow that approach, you will need to make these values comparable. For example, in this case, take the anti-log of residuals from the regressions that use logarithmic sale.\n\n\n\n\n7.1.8 Task 8\nConduct the conventional misspecification tests on the last model estimated.\n\n7.1.8.1 Guidance\nWe may start with the normality of the residuals. For this test, we will be using the jarque.bera.test() from the tseries package.\n\n# Normality of residuals\njarque.bera.test(df$residuals_m3)\n\n\n    Jarque Bera Test\n\ndata:  df$residuals_m3\nX-squared = 6.4227, df = 2, p-value = 0.0403\n\n\nThe null hypothesis of normal distribution is rejected at 5% significance level.\nFor the tests that follow, we will using the lmtest package.\nAutocorrelation Test\nWe use the bgtest() function below. It performs the Breusch-Godfrey Test. We first test for the first order autocorrelation and then, because we have quarterly data, the existence of autocorrelation up to order 4.\n\n# Autocorrelation\nbgtest(model_3)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  model_3\nLM test = 60.429, df = 1, p-value = 7.628e-15\n\nbgtest(model_3, order = 4)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  model_3\nLM test = 62.487, df = 4, p-value = 8.703e-13\n\n\nThere is autocorrelation problem in our model.\nHeteroscedasticity\nWe will use bptest() function for heteroscedasticity. It performs the Breusch-Pagan Test.\n\n# Heteroscedasticity\nbptest(model_3)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_3\nBP = 9.4108, df = 4, p-value = 0.05161\n\n\nThe null of no heteroscedasticity cannot be rejected at 5% significance level.\nFunctional Form\nWe will use resettest() for Ramsey’s RESET.\n\n# Ramsey RESET\nresettest(model_3)\n\n\n    RESET test\n\ndata:  model_3\nRESET = 36.442, df1 = 2, df2 = 73, p-value = 1.06e-11\n\n\nThe null of correct functional form is rejected.\n\n\nThe estimated model suffers from autocorrelation, heteroscedasticity, functional misspecification, and a structural break at the third quarter of 2001. Also, the residuals are non-normally distributed.\n\nPlease see your textbook for explanations of  possible implications of each of these misspecification.\n\n\n\n\n\n7.1.9 Task 9\nUsing the last model, forecast the sales value for each quarter of 2005.\n\n7.1.9.1 Guidance\nThere are more advanced ways of producing forecasts in R. But we need at this stage is explained below.\nDefine a forecast_2005 function using the coefficients of model_3. Let us see what model_3 coefficients are\n\nsummary(model_3)\n\n\nCall:\nlm(formula = ln_sales ~ Time + quarter2 + quarter3 + quarter4, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41512 -0.06014 -0.00347  0.09422  0.23885 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.882418   0.042709 278.219  &lt; 2e-16 ***\nTime         0.044620   0.000707  63.110  &lt; 2e-16 ***\nquarter2     0.013792   0.046130   0.299 0.765783    \nquarter3     0.184808   0.046146   4.005 0.000145 ***\nquarter4     0.367414   0.046173   7.957 1.44e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1459 on 75 degrees of freedom\nMultiple R-squared:  0.9822,    Adjusted R-squared:  0.9812 \nF-statistic:  1032 on 4 and 75 DF,  p-value: &lt; 2.2e-16\n\n\nDefine a function to forecast future values:\n\nforecast_2005 &lt;- function(Time, quarter2, quarter3, quarter4) {\n  exp(11.882418 + 0.044620 * Time + 0.013792 * quarter2 + 0.184808 * quarter3 + 0.367414 * quarter4)\n}\n\nUse the above function for forecasts.\n\n# Forecasts from 2005\n# quarter 1\ny2005_q1 &lt;- forecast_2005(81,0,0,0)\nprint(y2005_q1)\n\n[1] 5371609\n\n# quarter 2\ny2005_q2 &lt;- forecast_2005(81,1,0,0)\nprint(y2005_q2)\n\n[1] 5446207\n\n# quarter 3\ny2005_q3 &lt;- forecast_2005(81,0,1,0)\nprint(y2005_q3)\n\n[1] 6461978\n\n# quarter 4\ny2005_q4 &lt;- forecast_2005(81,0,0,1)\nprint(y2005_q4)\n\n[1] 7756579\n\n\n\n\n\n\nKleiber, C., and A. Zeileis. 2008. Applied Econometrics with r. Springer.\n\n\nWilson, J. H., and B. Keating. 2007. Business Forecasting. 5th edition. McGraw-Hill.",
    "crumbs": [
      "Seminar 4 (11 February 2025)",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "non-stationarity.html",
    "href": "non-stationarity.html",
    "title": "8  Unit Root (Non-stationary Time Series): Augmented Dickerm(y-Fuller Test",
    "section": "",
    "text": "8.1 Example: Pepper Price\nThe Pepper Price example provided in this section is taken from (Kleiber and Zeileis 2008).\nWe start by loading the required libraries.\nlibrary(AER) # Applied Econometrics with R, Kleiber and Zeileis, 2008\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\nlibrary(tseries) #Required for the adf test\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(vars)\n\nLoading required package: MASS\n\n\nLoading required package: strucchange\n\n\nLoading required package: urca\n\nlibrary(urca)\nLoad Pepper Price time series data and check the first 6 rows of observations.\ndata(\"PepperPrice\") \nhead(PepperPrice) \n\n        black   white\n[1,]  884.050 1419.78\n[2,]  919.329 1503.55\n[3,]  930.350 1536.62\n[4,] 1102.310 1629.22\n[5,] 1150.810 1737.24\n[6,] 1093.490 1629.22\nThere are two series here: black pepper and white pepper. Let’s understand the time series components better:\n# tsp stands for \"Time Series Properties\"\ntsp(PepperPrice)\n\n[1] 1973.75 1996.25   12.00\ntsp above stands for time series properties. It seems like we have monthly data (frequency of 12), starting in year 1973 and ending in year 1996.\nWe can use the window() function to inspect the values of the variables by setting start and end dates. In the example below, we start from the start of the sample and display values up until the 6th data point of 1974.\nwindow(PepperPrice, end = c(1974, 6))\n\n            black   white\nOct 1973  884.050 1419.78\nNov 1973  919.329 1503.55\nDec 1973  930.350 1536.62\nJan 1974 1102.310 1629.22\nFeb 1974 1150.810 1737.24\nMar 1974 1093.490 1629.22\nApr 1974 1117.740 1620.40\nMay 1974 1168.450 1671.11\nJun 1974 1117.740 1578.51",
    "crumbs": [
      "Seminar 5 (18 February 2025)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Root (Non-stationary Time Series): Augmented Dickerm(y-Fuller Test</span>"
    ]
  },
  {
    "objectID": "non-stationarity.html#example-pepper-price",
    "href": "non-stationarity.html#example-pepper-price",
    "title": "8  Unit Root (Non-stationary Time Series): Augmented Dickerm(y-Fuller Test",
    "section": "",
    "text": "8.1.1 Task 1\nChange the names of the variables black and white to black_pepper and white_pepper.\n\ncolnames(PepperPrice) &lt;- c(\"black_pepper\", \"white_pepper\")\n\n\n\n8.1.2 Task 2\nProvide a time-series plot of the white_pepper and black_pepper prices.\nLet us start by plotting the data:\n\nplot(PepperPrice, plot.type = \"single\", col = 1:2)\nlegend(\"topleft\", c(\"black_pepper\", \"white_pepper\"), \n       bty = \"n\", col = 1:2, lty = rep(1,2))\n\n\n\n\n\n\n\n\n\nplot(PepperPrice, ...) is the base R plot function for time series objects.\nplot.type = \"single\" ensures that multiple time series within PepperPrice are plotted on the same graph (rather than separate subplots).\ncol = 1:2 assigns different colors to the time series (we use R defaults above)\n\nThe second line after plot is about legend.\n\nlegend(\"topleft\", ...) places the legend in the top-left corner of the plot.\nc(\"black\", \"white\") are the legend labels for the two time series.\nbty = \"n\" removes the legend box (makes it look cleaner).\ncol = 1:2 matches the line colors (black and red).\nlty = rep(1,2) sets line type to solid (lty = 1) for both series.\n\n\n\n8.1.3 Task 3\nFind the order of integration of white_pepper and black_pepper prices.\n\n8.1.3.1 Guidance\nApply Dickey Fuller test without trend in test regression\n\nadf.test(PepperPrice[, \"white_pepper\"]) \n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PepperPrice[, \"white_pepper\"]\nDickey-Fuller = -1.6001, Lag order = 6, p-value = 0.7444\nalternative hypothesis: stationary\n\n\n\nadf.test(PepperPrice[, \"black_pepper\"]) \n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PepperPrice[, \"black_pepper\"]\nDickey-Fuller = -1.6434, Lag order = 6, p-value = 0.7262\nalternative hypothesis: stationary\n\n\nWe cannot reject the null hypothesis of unit root.\nLet us apply Augmented Dickey-Fuller (ADF) Test with 12 lags (because of the monthly frequency of the data, it likely to observe Autocorrelation up to 12 lags). Here, we have selected the lag length with some intuition. We will use another R command to choose the optimum lag length with the help of AIC (Akaike Information Criterion).\n\nadf.test(PepperPrice[, \"white_pepper\"], k = 12) \n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PepperPrice[, \"white_pepper\"]\nDickey-Fuller = -2.5763, Lag order = 12, p-value = 0.3332\nalternative hypothesis: stationary\n\n\n\nadf.test(PepperPrice[, \"black_pepper\"], k = 12) \n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PepperPrice[, \"black_pepper\"]\nDickey-Fuller = -2.3677, Lag order = 12, p-value = 0.4211\nalternative hypothesis: stationary\n\n\n\nBoth series are still with a unit root. They are non-stationary.\nSince the plots did not show a clear deterministic trend, we can proceed by differencing. We re-apply the Augmented Dickey-Fuller Test on the first differenced series.\n\n\nadf.test(diff(PepperPrice[, \"white_pepper\"])) \n\nWarning in adf.test(diff(PepperPrice[, \"white_pepper\"])): p-value smaller than\nprinted p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(PepperPrice[, \"white_pepper\"])\nDickey-Fuller = -5.8575, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nadf.test(diff(PepperPrice[, \"black_pepper\"])) \n\nWarning in adf.test(diff(PepperPrice[, \"black_pepper\"])): p-value smaller than\nprinted p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(PepperPrice[, \"black_pepper\"])\nDickey-Fuller = -4.973, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nThe differenced series are statonary for both indicators.\n\n\n\n\n8.1.4 Task 4\nPlot the first differenced series\n\n# Plot the first differenced white pepper price\nplot(diff(PepperPrice[, \"white_pepper\"]), \n     xlab = \"Year\", ylab = \"First difference of Price\", main = \"First-differenced White Pepper Prices\")\n\n\n\n\n\n\n\n\n\n# Plot the first differenced black pepper price\nplot(diff(PepperPrice[, \"black_pepper\"]), \n     xlab = \"Year\", ylab = \"First difference of Price\", main = \"First-differenced Black Pepper Prices\")\n\n\n\n\n\n\n\n\nWe can also see from the above plots that the first differenced series are stationary.\n\nThe white and black pepper prices are integrated of order 1, i.e. they are \\(I(1)\\).\n\n\n\n8.1.5 Task 5\nRe-apply the ADF test by choosing the optimum lag length using AIC (Akaike Information Criterion). Set a maximum of 12 lags.\n\n8.1.5.1 Guidance\nWe use the vars R package to find the optimum lag length for our ADF test.\n\nlag_selection &lt;- VARselect(PepperPrice[, \"white_pepper\"], lag.max = 12, type = \"const\")\n# print the results\nhead(lag_selection)\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    12      2      2     12 \n\n$criteria\n                 1           2           3           4           5           6\nAIC(n)    10.88957    10.80854    10.81078    10.81678    10.82324    10.82629\nHQ(n)     10.90061    10.82510    10.83286    10.84439    10.85637    10.86494\nSC(n)     10.91704    10.84973    10.86571    10.88544    10.90564    10.92242\nFPE(n) 53614.21642 49441.05649 49552.12848 49850.44270 50174.03385 50327.43278\n                 7           8           9          10          11          12\nAIC(n)    10.81944    10.82684    10.81711    10.79352    10.79293    10.78408\nHQ(n)     10.86362    10.87654    10.87233    10.85425    10.85919    10.85586\nSC(n)     10.92931    10.95044    10.95444    10.94458    10.95773    10.96260\nFPE(n) 49984.31397 50355.88525 49868.81434 48706.52773 48678.78476 48250.47433\n\n\n\n# Extract suggested lag from AIC criterion\noptimum_lag &lt;- lag_selection$selection[\"AIC(n)\"]\nprint(optimum_lag)\n\nAIC(n) \n    12 \n\n\n\n# Run ADF test with the selected lag\nadf.test(PepperPrice[, \"white_pepper\"], k = optimum_lag)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PepperPrice[, \"white_pepper\"]\nDickey-Fuller = -2.5763, Lag order = 12, p-value = 0.3332\nalternative hypothesis: stationary\n\n\nYou may replicate the above for the black pepper price and also for the first-differenced series.\nAlso, you may replicate the above analysis using logarithmic series.\n\n\n\n8.1.6 Task 6\nReplicate the ADF test including trend in the test regression.\nPlease note that this test is only for exercise purposes (to show how it could be done in R).The plot of our white and pepper price series do not suggest existence of a deterministic trend. Hence, the results obtained below will not be used in the analysis further.\n\n8.1.6.1 Guidance\nWe use R’s urca library.\n\n# library(urca)\n# Run the ADF test with trend\nadf_trend &lt;- ur.df(PepperPrice[, \"white_pepper\"], type = \"trend\", lags = 1)\n\n# Print test results\nsummary(adf_trend)\n\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-952.60  -92.97  -18.41   65.97 1155.13 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.491863  34.431212   1.466   0.1437    \nz.lag.1     -0.017740   0.009639  -1.841   0.0668 .  \ntt           0.043479   0.177088   0.246   0.8062    \nz.diff.lag   0.290817   0.058798   4.946 1.35e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 218.1 on 265 degrees of freedom\nMultiple R-squared:  0.09145,   Adjusted R-squared:  0.08117 \nF-statistic: 8.892 on 3 and 265 DF,  p-value: 1.235e-05\n\n\nValue of test-statistic is: -1.8406 1.2023 1.7202 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.98 -3.42 -3.13\nphi2  6.15  4.71  4.05\nphi3  8.34  6.30  5.36\n\n# type can take on: none; drift; trend\n\nIn the next section, we will check for a cointegrating relationship between these variables.\n\n\n\n\nKleiber, C., and A. Zeileis. 2008. Applied Econometrics with r. Springer.",
    "crumbs": [
      "Seminar 5 (18 February 2025)",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Unit Root (Non-stationary Time Series): Augmented Dickerm(y-Fuller Test</span>"
    ]
  },
  {
    "objectID": "cointegration.html",
    "href": "cointegration.html",
    "title": "9  Cointegration: Engle-Granger Test",
    "section": "",
    "text": "9.1 Example: Pepper Price\nWe continue from the previous section, where we concluded that the white and black pepper prices are \\(I(1)\\).\nIn this section, we test whether they have a cointegrating relationship.\n&lt;div style=“color:green&gt;\nThere are two conditions for cointegration:\nIn the previous section, we established that the series are integrated of the same order. The next task finds out whether they have a stationary linear relationship.",
    "crumbs": [
      "Seminar 5 (18 February 2025)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cointegration: Engle-Granger Test</span>"
    ]
  },
  {
    "objectID": "cointegration.html#example-pepper-price",
    "href": "cointegration.html#example-pepper-price",
    "title": "9  Cointegration: Engle-Granger Test",
    "section": "",
    "text": "The time-series should be integrated of the same order\nThere is a stationary linear relationship between the variables.",
    "crumbs": [
      "Seminar 5 (18 February 2025)",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cointegration: Engle-Granger Test</span>"
    ]
  },
  {
    "objectID": "data-visualisation-ggplot2.html",
    "href": "data-visualisation-ggplot2.html",
    "title": "10  Data Visualisation Using ggplot2",
    "section": "",
    "text": "10.1 Example 1: Scatter plot with wage data\nThe exercises below are a selection from the “introduction to regression analysis” section. Load necessary libraries\nlibrary(readxl)\nlibrary(ggplot2)",
    "crumbs": [
      "Seminar 6 (25 February 2025)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation-ggplot2.html#example-1-scatter-plot-with-wage-data",
    "href": "data-visualisation-ggplot2.html#example-1-scatter-plot-with-wage-data",
    "title": "10  Data Visualisation Using ggplot2",
    "section": "",
    "text": "10.1.1 Task 1\n\n10.1.1.1 Task\nImport wage.xls data. Alternatively, you may download and work on the wage-analysis project file.\n\n\n10.1.1.2 Guidance\nUse read_excel() and head() functions.\n\n# install.packages(\"readxl\")\n#library(readxl)\n\n# Import Excel data\nwage2 &lt;- read_excel(\"./assets/data/wage2.xls\", sheet = \"wage2\")\n\n\n\n\n10.1.2 Task 2\n\n10.1.2.1 Task\nExamine the relationship between education and wage using a scatter plot.\n\n\n10.1.2.2 Guidance\nWe use the ggplot2 package to draw plots.\nEducation is expected to have a positive impact on wage. In our scatter plot, educ will be on the horizontal-axis while wage will be on the vertical-axis.\n\n# Scatter plot\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  labs(title = \"Scatter plot of Wage vs. Education\", x = \"Years of Schooling\", y = \"Wage\")\n\n\n\n\n\n\n\n\nYou see above the full set of lines to create this plot. But let us do this step by step to have a better understanding. First, we bring the educ and wage variables from the wage2 data and position these on our plot.\n\nggplot(wage2, aes(x = educ, y = wage))\n\n\n\n\n\n\n\n\nWe then add (using the + sign), the observations in our data, represented by dots.\n\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() \n\n\n\n\n\n\n\n\nIt is always good practice to give a title for your plot. Notice also that the horizontal and vertical axes above are labelled by the variable names. We may also replace these with proper definitions of the variables. This is to make it easier for the readers to understand your plots:\n\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  labs(title = \"Scatter plot of Wage vs. Education\", x = \"Years of Schooling\", y = \"Wage\")\n\n\n\n\n\n\n\n\n\n\n\n10.1.3 Task 3\nFit a regression line to the scatterplot you created\n\n10.1.3.1 Guidance\nWe will be adding the regression line to the scatter plot we produced above.\n\n# Scatter plot with fitted line\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nthe geom_smooth(method = \"lm\") asks R to add a line estimating a “linear model” (i.e. a regression) of wage on educ.\nNote that we could save this plot as an object by assigning it a name on the left hand side of the command. We will do that below and name the plot as scatter_wage_educ.\nCan you guess what the plot would look if we changed se = FALSE to se = TRUE above? We can also try that below:\n\n# Scatter plot with fitted line\nscatter_wage_educ &lt;- ggplot(wage2, aes(x = educ, y = wage)) +\n                        geom_point() +\n                        geom_smooth(method = \"lm\", se = TRUE) +\n                        labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\nprint(scatter_wage_educ)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe could also add this sample regression line by saving predictions after estimation of a wage regression and using these predictons.\nWe use the lm() function to estimate linear regression models.\n\n# Linear regression\nmodel_1 &lt;- lm(wage ~ educ, data = wage2)\nsummary(model_1)\n\n\nCall:\nlm(formula = wage ~ educ, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-877.38 -268.63  -38.38  207.05 2148.26 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  146.952     77.715   1.891   0.0589 .  \neduc          60.214      5.695  10.573   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 382.3 on 933 degrees of freedom\nMultiple R-squared:  0.107, Adjusted R-squared:  0.106 \nF-statistic: 111.8 on 1 and 933 DF,  p-value: &lt; 2.2e-16\n\n\nUsing the regression model above, we can predict what the wage would be for given values of education (how much do we expect the wage would be for given years of schooling).\n\n# Save predicted values under name wage_hat\nwage2$wage_hat &lt;- predict(model_1)\n\nWe can now add the estimated regression line to the wage-education scatter plot.\n\n# Scatter plot with fitted line\n# we add the wage_hat variable\nggplot(wage2, aes(x = educ, y = wage)) +\n  geom_point() +\n  geom_line(aes(y = wage_hat), color = \"blue\", size = 1) +\n  labs(title = \"Scatter plot with Fitted Line\", x = \"Years of Schooling\", y = \"Wage\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nNote that we used geom_line() this time to add a line plot of an already existing variable in the data set.\n\nggplot(wage2, aes(x = educ, y = wage)) creates a canvas, a plot area with educ at the horizontal and wage at the vertical axis\ngeom_point() adds a scatterplot of wage against educ.\ngeom_line(aes(y = wage_hat)) adds the line for the predicted wage_hat values. The aes(y = wage_hat) ensures the line graph uses wage_hat on the y-axis while sharing the x-axis (educ).\ncolor and size are optional for styling the line. Try experimenting with these and observe the changes.",
    "crumbs": [
      "Seminar 6 (25 February 2025)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation-ggplot2.html#example-2-histogram-of-error-term-using-wage2-data",
    "href": "data-visualisation-ggplot2.html#example-2-histogram-of-error-term-using-wage2-data",
    "title": "10  Data Visualisation Using ggplot2",
    "section": "10.2 Example 2: Histogram of error term using wage2 data",
    "text": "10.2 Example 2: Histogram of error term using wage2 data\nThe following example is taken from “multiple regression and diagnostic checks” section.\n\n10.2.1 Task 4\nEstiamte a multiple regression of wage using IQ, educ, exper, urban. Provide a histogram of the error terms.\n\n10.2.1.1 Guidance\n\nmodel_3 &lt;- lm(wage ~ IQ + educ + exper + urban, data = wage2) \nwage2$resid_m3 &lt;- residuals(model_3)\n\nPlot the residuals to see the distribution.\n\nggplot(wage2, aes(x = resid_m3)) + \n  geom_histogram(binwidth = 200, fill = \"skyblue\", color = \"black\") + \n  labs(title = \"Histogram of Residuals (model_3)\", x = \"Residuals\", y = \"Frequency\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\naes(x = resid) specifies the residuals as the variable for the x-axis.\ngeom_histogram() is used to create the histogram:\n\nbinwidth = 200 controls the width of the bins. You can adjust this depending on how detailed you want the histogram to be.\nfill sets the color inside the bars, and color adds a border around them for better visibility.\n\nlabs() adds labels for the title and axes.\ntheme_minimal() gives a clean, simple look to the plot - try the plot with and without this.\n\nYou may also let ggplot choose the number of bins automatically:\n\nggplot(wage2, aes(x = resid_m3)) +\n  geom_histogram(fill = \"pink\", color = \"black\") +\n  labs(title = \"Histogram of Residuals (model_3)\", x = \"Residuals\", y = \"Frequency\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Seminar 6 (25 February 2025)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "data-visualisation-ggplot2.html#example-3-line-plots-using-gap_sales-data",
    "href": "data-visualisation-ggplot2.html#example-3-line-plots-using-gap_sales-data",
    "title": "10  Data Visualisation Using ggplot2",
    "section": "10.3 Example 3: Line plots using Gap_sales data",
    "text": "10.3 Example 3: Line plots using Gap_sales data\nThe following examples are from the “introduction to time series analysis” section.\nStart by reading the data\n\ndf &lt;- read.csv(\"~/Desktop/R-workshops/assets/data/GAP_Sales.csv\", stringsAsFactors=TRUE)\n\n\nggplot(df, aes(x = Time, y = Sales)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou may add some color and change the size of the line\n\nggplot(df, aes(x = Time, y = Sales)) +\n  geom_line(color = \"blue\", size = 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn the above plot, although we can see the pattern of the Sales variable quite clearly, the Time variable labels fail to show us the respective quarter values. We may change these labels by using the following lines of code.\nWe first define labels to correspond to each data point\n\n# First, create a new column for formatted quarter labels\ndf$Quarter_label &lt;- paste0(df$Year, \":\", df$quarter)\n\nCheck the values of Quarter_label in the df. You will see that it goes on like 1985:q1, 1985:q2, and so on. We may now use these labels instead of the values of the Time variable.\n\nggplot(df, aes(x = Time, y = Sales)) +\n  geom_line(color = \"blue\", size = 1) +\n  scale_x_continuous(\n    breaks = df$Time,  # Position the breaks at each quarter, i.e. at each value of Time\n    labels = df$Quarter_label  # Label each point using Quarter_label variable created above\n  ) + # provide a title and axes labels below\n  labs(title = \"Quarterly GAP Sales\", x = \"Quarter\", y = \"Sales\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, size=6))  # Rotate labels for better readability using the angle option and set the font size for label using size option\n\n\n\n\n\n\n\n\n\n# Estimate the model using trend and quarter dummies\ndf$ln_sales = log(df$Sales)\nmodel_3 &lt;- lm(ln_sales ~ Time + Q2 + Q3 + Q4, data = df)\n# Obtain predictions from model_3\ndf$ln_sales_hat_m3 &lt;- predict(model_3)\n\nPlot the actual values against predictions to see the improvement by the inclusion of the quarter\n\n# Plot actual versus predicted log sales\nggplot(df, aes(x = Time)) +\n  geom_line(aes(y = ln_sales, color = \"Actual\")) +\n  geom_line(aes(y = ln_sales_hat_m3, color = \"Predicted\")) +\n  theme_minimal() +\n  labs(title = \"Actual versus predicted logarithmic sales\", x = \"Time\", y = \"logarithmic sales\")\n\n\n\n\n\n\n\n\nChange the color of the lines\n\nggplot(df, aes(x = Time)) +\n  geom_line(aes(y = ln_sales, color = \"Actual\")) +\n  geom_line(aes(y = ln_sales_hat_m3, color = \"Predicted\")) +\n  scale_color_manual(values = c(\"Actual\" = \"blue\", \"Predicted\" = \"red\")) +\n  theme_minimal() +\n  labs(title = \"Actual versus predicted logarithmic sales\", x = \"Time\", y = \"logarithmic sales\")",
    "crumbs": [
      "Seminar 6 (25 February 2025)",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualisation Using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "introduction-to-panel-data-analysis.html",
    "href": "introduction-to-panel-data-analysis.html",
    "title": "11  Introduction to Panel Data Analysis",
    "section": "",
    "text": "11.1 Example: European Countries Gasoline Consumption Data\nData is obtained from Abay Mulatu 316ECN Applied Econometrics lecture material, Coventry University.\nWe start by loading the required libraries.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(Hmisc) # add labels to variables\n\n\nAttaching package: 'Hmisc'\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\nlibrary(ggplot2)\nlibrary(dplyr)\nWe will start by looking a a sub-group of two countries: Italy and Denmark and then move on to do estimations using the complete data on 18 countries.",
    "crumbs": [
      "Seminar 6 (25 February 2025)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Panel Data Analysis</span>"
    ]
  },
  {
    "objectID": "introduction-to-panel-data-analysis.html#example-european-countries-gasoline-consumption-data",
    "href": "introduction-to-panel-data-analysis.html#example-european-countries-gasoline-consumption-data",
    "title": "11  Introduction to Panel Data Analysis",
    "section": "",
    "text": "11.1.1 Task 1\nImport the data into R.\n\ndf &lt;- read.csv(\"~/Desktop/R-workshops/assets/data/gasoline-demand-IT-DK.csv\", stringsAsFactors=TRUE)\n#View(df)\n\nFor ease of typing, I called this data as df. In the code below, df will refer to the gasoline-demand-IT-DK data we imported.\n\n\n11.1.2 Task 2\nLabel variables.\n\n# library(Hmisc)\n\nlabel(df$L_gas_cons_pcar) &lt;- \"Logarithm of gasoline consumption per car\"\nlabel(df$L_income_pc) &lt;- \"Logarithm of real income per capita\"\nlabel(df$L_gas_price) &lt;- \"Logarithm of real gasoline price per gallon\"\nlabel(df$L_cars_pc) &lt;- \"Logarithm of number of cars per capita\"\n\n\n\n11.1.3 Task 3\nProduce a scatter plot of gasoline consumption by car versus income per capita separately for Italy and Denmark.\n\n11.1.3.1 Guidance\nLet’s first create data frames for Italy and Denmark.\n\ndf_Italy &lt;- df %&gt;% \n  filter(country == \"ITALY\")\n\ndf_Denmark &lt;- df %&gt;% \n  filter(country == \"DENMARK\")\n\nCreate scatterplots using dplyr\n\nggplot(df_Italy, aes(x = L_income_pc, y = L_gas_cons_pcar)) +\n  geom_point() +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Italy\" )\n\n\n\n\n\n\n\n# Save the plot\n# ggsave(\"./plots/panel-data-analysis/italy.png\")\n\nWe can do the same for Denmark\n\nggplot(df_Denmark, aes(x = L_income_pc, y = L_gas_cons_pcar)) +\n  geom_point() +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark\" )\n\n\n\n\n\n\n\n# Save the plot\n# ggsave(\"./plots/panel-data-analysis/denmark.png\")\n\nBoth countries depict a negative relationship between income per capita and gasoline consumption by car. As the income per capita of Italy or Denmark increases, the gasoline consumption per car declines. What could be the reason of this pattern? Please provide a reasonable explanation.\n\n\n\n11.1.4 Task 4\nNow, rather than focussing on each country separately, let’s work with a panel data of these two countries.\nProvide a scatter plot of the two indicators in both countries. Assign different colors to each country data points.\nLet’s first draw the scatter plot\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar)) +\n  geom_point() +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\" )\n\n\n\n\n\n\n\n\nAdd a regression line to the data points above:\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\" )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# ggsave(\"./plots/panel-data-analysis/pooled-fit.png\")\n\nThe above plot fit an OLS regression line to the data points in our scatter plot. But it seems like something is wrong. Separate scatter plots for Denmark and Italy revealed a negative relationship between log income per capita and log gasoline consumption per car whereas the above regression line suggests a positive relationship. Which one is correct?\nLet us dive into this deeper. First, let us see which data points belong to which country and then move on from there.\nIn the scatter plot, add separate colors for each country\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar)) +\n  geom_point(aes(color = country)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggsave(\"./plots/panel-data-analysis/pooled-color-fit.png\")\n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWe can see a clear separation of Denmark and Italy. If we were to use separate country samples, rather than a pooled sample of the two, we would fit the regression lines below. Let’s start with the scatter and then add the regression lines:\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar, color = country)) +\n  geom_point() +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\")\n\n\n\n\n\n\n\n# ggsave(\"./plots/panel-data-analysis/italy-denmark.png\")\n\nAdd regression lines to each plot\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar, color = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# ggsave(\"./plots/panel-data-analysis/italy-denmark-fits.png\")\n\nSo, which pattern is correct?!\n\n\n11.1.5 Task 5\nEstimate a pooled regression of gasoline consumption on income per capita.\nWe will be using our lm() function as we have done before:\n\npooled_ols &lt;- lm(L_gas_cons_pcar ~ L_income_pc, data = df)\nsummary(pooled_ols)\n\n\nCall:\nlm(formula = L_gas_cons_pcar ~ L_income_pc, data = df)\n\nResiduals:\nLogarithm of gasoline consumption per car \n     Min       1Q   Median       3Q      Max \n-0.56567 -0.14980  0.02645  0.20870  0.54445 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.6079     0.8004   7.006 3.22e-08 ***\nL_income_pc   0.2723     0.1320   2.063   0.0464 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2878 on 36 degrees of freedom\nMultiple R-squared:  0.1057,    Adjusted R-squared:  0.08085 \nF-statistic: 4.254 on 1 and 36 DF,  p-value: 0.04642\n\n\nAccording to the results above, 1% increase in income per capita increases the gasoline consumption per car by 0.27%, on average.\n\n\n11.1.6 Task 6\nCreate dummy variables representing each country in the sample and run the same regression above, this time including a country dummy (in the context of panel data anlysis, we will refer to this as the “country fixed effect”).\nCreate country dummies using ifelse() function.\n\ndf$italy &lt;- ifelse(df$country == \"ITALY\", 1, 0)\ndf$denmark &lt;- ifelse(df$country == \"DENMARK\", 1, 0)\n\nRe-run the above regression with denmark dummy. Can you explain why we do not include both italy and denmark but choose to include one of these countries only?\n\nlsdv &lt;- lm(L_gas_cons_pcar ~ L_income_pc + denmark, data = df)\nsummary(lsdv)\n\n\nCall:\nlm(formula = L_gas_cons_pcar ~ L_income_pc + denmark, data = df)\n\nResiduals:\nLogarithm of gasoline consumption per car \n      Min        1Q    Median        3Q       Max \n-0.121945 -0.038747 -0.001717  0.035970  0.101286 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.14743    0.31063  -6.913 4.95e-08 ***\nL_income_pc -0.92547    0.04887 -18.937  &lt; 2e-16 ***\ndenmark      1.00963    0.03457  29.205  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05795 on 35 degrees of freedom\nMultiple R-squared:  0.9647,    Adjusted R-squared:  0.9627 \nF-statistic: 478.9 on 2 and 35 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that in Denmark, on average, gasoline consumption per car is around 174% higher than in Italy, holding income per capita levels constant. Can you figure out how did we get that number?\nLooking at the coefficient of L_income_pc, we can say that a 1% increase in income per capita, on average, decreases the gasoline consumption per car by around 0.93%. This is a very different figure than what we obtained using pooled OLS.\nThe model we estimated above using country dummies is using the Least Squares Dummy Variable approach.\n\n\n11.1.7 Task 7\nPlot predictions from the model estimated above.\n\ndf$lsdv_hat &lt;- predict(lsdv)\n\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar, color = country)) +\n  geom_point() +\n  geom_line(aes(y = lsdv_hat)) +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\")\n\n\n\n\n\n\n\n# ggsave(\"./plots/panel-data-analysis/italy-denmark-lsdv.png\")\n\nThe LSDV approach assumes a command slope for Italy and Denmark, but captures the constant distance between them through a country dummy. Note that the slope coefficient will give us the average effect for all countries in sample and will be different than the coefficients we would obtain if we were to estimate separate regressions for each country.\nHow does this compare to individual regressions (i.e. if we were to estimate each separately?). In the case of this example, we get very similar slope coefficients. But we will see on a larger sample that this is not always the case.\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar, color = country)) +\n  geom_point() +\n  geom_line(aes(y = lsdv_hat, color = \"lsdv_hat\")) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"log of income per capita\", y = \"log of gasoline consumption per car\", title = \"Income and gasoline consumption relationship for Denmark and Italy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# ggsave(\"./plots/panel-data-analysis/italy-denmark-lsdv.png\")\n\n\n\n11.1.8 Task 8\nFor L_gas_cons_pcar and L_income_pc variables, create deviations from within-group averages.\n\n11.1.8.1 Guidance\nWe can use the dplyr package. We first create within group averages\n\n# library(dplyr)\ndf &lt;- df %&gt;%\n  group_by(country) %&gt;%\n  mutate(m_L_gas_cons_pcar = mean(L_gas_cons_pcar, na.rm = TRUE),\n         m_L_income_pc = mean(L_income_pc, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nView the data to see what we have done above\n\n#View(df[, c(\"country\", \"year\", \"L_gas_cons_pcar\", \"m_L_gas_cons_pcar\")])\n\nCreate deviations from the mean:\n\ndf &lt;- df %&gt;% \n  mutate(wi_L_gas_cons_pcar = L_gas_cons_pcar - m_L_gas_cons_pcar,\n         wi_L_income_pc = L_income_pc - m_L_income_pc)\n\nPlot deviations from mean\n\nggplot(df, aes(x = wi_L_income_pc, y = wi_L_gas_cons_pcar, color = country)) + \n       geom_point()\n\n\n\n\n\n\n\n# ggsave(\"./plots/panel-data-analysis/deviations-from-mean.png\")\n\n\n\n\n11.1.9 Task 9\nRun an OLS regression on the deviations from group averages without a constant.\n\n11.1.9.1 Guidance\nThis is called the within-groups estimator. The slope coefficient that we obtain for our explanatory variable will be the same as the one obtained from LSDV approach, with a small difference in standard error estimates.\n\nwg &lt;- lm(wi_L_gas_cons_pcar ~ 0 + wi_L_income_pc, data = df)\nsummary(wg)\n\n\nCall:\nlm(formula = wi_L_gas_cons_pcar ~ 0 + wi_L_income_pc, data = df)\n\nResiduals:\nLogarithm of gasoline consumption per car \n      Min        1Q    Median        3Q       Max \n-0.121945 -0.038747 -0.001717  0.035970  0.101286 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \nwi_L_income_pc -0.92547    0.04753  -19.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05636 on 37 degrees of freedom\nMultiple R-squared:  0.9111,    Adjusted R-squared:  0.9087 \nF-statistic: 379.1 on 1 and 37 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Seminar 6 (25 February 2025)",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Panel Data Analysis</span>"
    ]
  },
  {
    "objectID": "panel-data-models.html",
    "href": "panel-data-models.html",
    "title": "12  Panel Data Models",
    "section": "",
    "text": "12.1 Least Squares Dummy Variables (LSDV) Approach\nWe estimated this model in the previous section using two-country sample of gasoline consumption data. We will replicate it with full sample of countries.",
    "crumbs": [
      "Seminar 7 (4 March 2025)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Panel Data Models</span>"
    ]
  },
  {
    "objectID": "panel-data-models.html#least-squares-dummy-variables-lsdv-approach",
    "href": "panel-data-models.html#least-squares-dummy-variables-lsdv-approach",
    "title": "12  Panel Data Models",
    "section": "",
    "text": "12.1.1 Task 1\nImport the gasoline-demand-all-countries.csv data, label variables and create country dummies.\n\n12.1.1.1 Guidance\nThis is a replication of what we have done in the previous section.\n\ndf &lt;- read.csv(\"~/Desktop/R-workshops/assets/data/gasoline-demand-all-countries.csv\", stringsAsFactors=TRUE)\n#View(df)\n# label variables\nlabel(df$L_gas_cons_pcar) &lt;- \"Logarithm of gasoline consumption per car\"\nlabel(df$L_income_pc) &lt;- \"Logarithm of real income per capita\"\nlabel(df$L_gas_price) &lt;- \"Logarithm of real gasoline price per gallon\"\nlabel(df$L_cars_pc) &lt;- \"Logarithm of number of cars per capita\"\n# create country dummies\n# below is what we have done before\n# df$italy &lt;- ifelse(df$country == \"ITALY\", 1, 0)\n# df$denmark &lt;- ifelse(df$country == \"DENMARK\", 1, 0)\n\nBecause the data now has 18 countries, I will use another approach to create the country dummies:\n\ndf &lt;- dummy_cols(df, select_columns = \"country\", remove_first_dummy = FALSE, remove_selected_columns = FALSE)\n\nView(df) and you will see that country dummies are created with names country_AUSTRIA, country_BELGIUM, etc. You may remove the country_ in front of all these dummy country names and convert letters to lower case by running the following line:\n\ndf &lt;- df %&gt;%\n  rename_with(~ tolower(gsub(\"country_\", \"\", .)), starts_with(\"country_\"))\n\n\n\n\n12.1.2 Task 2\nProvide a scatter plot of L_gas_cons_pcar and L_income_pc. Differentiate each country data point by adding color.\n\nggplot(df, aes(x = L_income_pc, y = L_gas_cons_pcar, color = country)) +\n  geom_point() +\n  labs(x = \"Log income per capita\", y = \"Log gasoline consumption per car\",\n       title = \"Negative Relationship Between Income and Gasoline Consumption\")\n\n\n\n\n\n\n\n#ggsave(\"plots/panel-data-analysis/scatter-all-countries.png\")\n\n\n\n12.1.3 Task 3\nEstimate a Least Squares Dummy Variable Model that regresses logarithm of gasoline consumption per car (L_gas_cons_pcar) on log of income per capita (L_income_pc), log of gasoline price per gallon (L_gas_price) and number of cars per capita (L_cars_pc). Comment on the results.\n\n12.1.3.1 Guidance\nThere are multiple ways of approaching data analysis. We can use the country dummies that we created above or alternatively, we can ask R to add dummies based on our country variable. I will show the latter below. Try replicating this using dummies we created in the previous task.\nLeast Squares Dummy Variable Estimation:\n\n# Least Squares Dummy Variable\nlsdv &lt;- lm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + factor(country), data = df)\nsummary(lsdv)\n\n\nCall:\nlm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + \n    factor(country), data = df)\n\nResiduals:\nLogarithm of gasoline consumption per car \n     Min       1Q   Median       3Q      Max \n-0.37877 -0.03976  0.00465  0.04541  0.36286 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 2.28586    0.22832  10.011  &lt; 2e-16 ***\nL_income_pc                 0.66225    0.07339   9.024  &lt; 2e-16 ***\nL_gas_price                -0.32170    0.04410  -7.295 2.35e-12 ***\nL_cars_pc                  -0.64048    0.02968 -21.580  &lt; 2e-16 ***\nfactor(country)BELGIUM     -0.12030    0.03415  -3.523 0.000489 ***\nfactor(country)CANADA       0.75598    0.04075  18.554  &lt; 2e-16 ***\nfactor(country)DENMARK      0.10360    0.03660   2.830 0.004944 ** \nfactor(country)FRANCE      -0.08108    0.03356  -2.416 0.016256 *  \nfactor(country)GERMANY     -0.13599    0.03188  -4.266 2.63e-05 ***\nfactor(country)GREECE       0.05125    0.04153   1.234 0.218049    \nfactor(country)IRELAND      0.30647    0.03529   8.683  &lt; 2e-16 ***\nfactor(country)ITALY       -0.05331    0.03711  -1.436 0.151868    \nfactor(country)JAPAN        0.09007    0.03861   2.333 0.020262 *  \nfactor(country)NETHERLANDS -0.05106    0.03358  -1.521 0.129280    \nfactor(country)NORWAY      -0.06916    0.04041  -1.711 0.087967 .  \nfactor(country)SPAIN       -0.60408    0.09122  -6.622 1.49e-10 ***\nfactor(country)SWEDEN       0.74049    0.18008   4.112 4.99e-05 ***\nfactor(country)SWITZERLAND  0.11665    0.03471   3.360 0.000872 ***\nfactor(country)TURKEY       0.22413    0.04764   4.704 3.79e-06 ***\nfactor(country)UK           0.05959    0.03019   1.974 0.049237 *  \nfactor(country)USA          0.76940    0.04458  17.260  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09233 on 321 degrees of freedom\nMultiple R-squared:  0.9734,    Adjusted R-squared:  0.9717 \nF-statistic: 586.6 on 20 and 321 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n12.1.4 Task 4\nTest if the the country dummies in the LSDV model above are jointly statistically significant.\n\n12.1.4.1 Guidance\nWe need to perform an F test for restrictions. The LSDV model estimated above is the unrestricted model. We may re-estimate a restricted version of it by removing the country dummies. This model is our pooled OLS.\n\npooled_ols &lt;- lm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, data = df)\nsummary(pooled_ols)\n\n\nCall:\nlm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n    data = df)\n\nResiduals:\nLogarithm of gasoline consumption per car \n     Min       1Q   Median       3Q      Max \n-0.38412 -0.15307 -0.04981  0.16529  0.59684 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.39133    0.11693   20.45   &lt;2e-16 ***\nL_income_pc  0.88996    0.03581   24.86   &lt;2e-16 ***\nL_gas_price -0.89180    0.03031  -29.42   &lt;2e-16 ***\nL_cars_pc   -0.76337    0.01861  -41.02   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.21 on 338 degrees of freedom\nMultiple R-squared:  0.8549,    Adjusted R-squared:  0.8536 \nF-statistic:   664 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\n\\(H_0 : \\beta_{belgium} = \\beta_{canada} = \\dots = \\beta_{usa} = 0\\)\n\\(H_1 : \\text{at least one is different than zero}\\)\nNow, compare the restricted model with the unrestricted one:\n\n# anova_r &lt;- anova(restricted_model, full_model)\nanova_r &lt;- anova(pooled_ols, lsdv)\n\nprint(anova_r)\n\nAnalysis of Variance Table\n\nModel 1: L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc\nModel 2: L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + factor(country)\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    338 14.9044                                  \n2    321  2.7365 17    12.168 83.961 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe reject \\(H_0\\) . The country dummies are jointly statistically significant.\n\n\n\n12.1.5 Task 5\nReplicate the above test, this time using R’s fixed effects estimation.\nplm is the function we will use to estimate a panel linear model, with the index showing our panel data cross-section and time identifiers and the model telling R which panel linear model to estimate:\n\n# Panel Data Fixed Effects Model\nfixed_1 &lt;- plm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n               data = df, index = c(\"country\", \"year\"), model = \"within\")\nsummary(fixed_1)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n    data = df, model = \"within\", index = c(\"country\", \"year\"))\n\nBalanced Panel: n = 18, T = 19, N = 342\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-0.378774 -0.039758  0.004650  0.045412  0.362856 \n\nCoefficients:\n             Estimate Std. Error  t-value  Pr(&gt;|t|)    \nL_income_pc  0.662250   0.073386   9.0242 &lt; 2.2e-16 ***\nL_gas_price -0.321702   0.044099  -7.2950 2.355e-12 ***\nL_cars_pc   -0.640483   0.029679 -21.5804 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    17.061\nResidual Sum of Squares: 2.7365\nR-Squared:      0.8396\nAdj. R-Squared: 0.82961\nF-statistic: 560.093 on 3 and 321 DF, p-value: &lt; 2.22e-16\n\n\nNote that the slope coefficients we obtain above are the same as those we obtained in our LSDV model.\nThe test between Fixed Effects and Pooled OLS models\n\npFtest(fixed_1, pooled_ols)\n\n\n    F test for individual effects\n\ndata:  L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc\nF = 83.961, df1 = 17, df2 = 321, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects\n\n\nAgain, note that the F-statistic reported above (83.961) is the same as what we obtained through anova() comparison.\n\n\n12.1.6 Task 6\nEstimate the Within-groups model manually.\n\n12.1.6.1 Guidance\nFor this, we will need to regress the deviations of observations from group means using a linear model without a constant.\nLet’s first create the group averages using the dplyr package\n\ndf &lt;- df %&gt;%\n  group_by(country) %&gt;%\n  mutate(m_L_gas_cons_pcar = mean(L_gas_cons_pcar, na.rm = TRUE),\n         m_L_income_pc = mean(L_income_pc, na.rm = TRUE),\n         m_L_gas_price = mean(L_gas_price, na.rm = TRUE),\n         m_L_cars_pc = mean(L_cars_pc, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nWe then find the deviations of each observation from the group average.\n\ndf &lt;- df %&gt;% \n  mutate(wi_L_gas_cons_pcar = L_gas_cons_pcar - m_L_gas_cons_pcar,\n         wi_L_income_pc = L_income_pc - m_L_income_pc,\n         wi_L_gas_price = L_gas_price - m_L_gas_price,\n        wi_L_cars_pc = L_cars_pc - m_L_cars_pc)\n\nIn the final step, we use these deviations in a regression without a constant term. But let us see the scatter plot of gasoline consumption per car and income per capita after this data transformation.\n\nggplot(df, aes(x = wi_L_income_pc, y = wi_L_gas_cons_pcar, color = country)) + \n  geom_point()\n\n\n\n\n\n\n\n\nRegression (within-groups estimation):\n\nwg &lt;- lm(wi_L_gas_cons_pcar ~ 0 + wi_L_income_pc + wi_L_gas_price + wi_L_cars_pc,\n         data = df)\nsummary(wg)\n\n\nCall:\nlm(formula = wi_L_gas_cons_pcar ~ 0 + wi_L_income_pc + wi_L_gas_price + \n    wi_L_cars_pc, data = df)\n\nResiduals:\nLogarithm of gasoline consumption per car \n     Min       1Q   Median       3Q      Max \n-0.37877 -0.03976  0.00465  0.04541  0.36286 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \nwi_L_income_pc  0.66225    0.07141   9.274  &lt; 2e-16 ***\nwi_L_gas_price -0.32170    0.04291  -7.497 5.77e-13 ***\nwi_L_cars_pc   -0.64048    0.02888 -22.177  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08985 on 339 degrees of freedom\nMultiple R-squared:  0.8396,    Adjusted R-squared:  0.8382 \nF-statistic: 591.5 on 3 and 339 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n12.1.7 Task 7\nEstimate a random effects model and compare this with Pooled OLS.\nTo estimate a panel random effect model, we use the same panel data linear model plm() function as above, but will change the model to random.\n\n# Panel Data Random Effects Model\nrandom_1 &lt;- plm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n               data = df, index = c(\"country\", \"year\"), model = \"random\")\nsummary(random_1)\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n    data = df, model = \"random\", index = c(\"country\", \"year\"))\n\nBalanced Panel: n = 18, T = 19, N = 342\n\nEffects:\n                   var  std.dev share\nidiosyncratic 0.008525 0.092330 0.182\nindividual    0.038238 0.195545 0.818\ntheta: 0.8923\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.3977058 -0.0520350  0.0050877  0.0582288  0.3763726 \n\nCoefficients:\n             Estimate Std. Error  z-value  Pr(&gt;|z|)    \n(Intercept)  1.996698   0.184326  10.8324 &lt; 2.2e-16 ***\nL_income_pc  0.554986   0.059128   9.3861 &lt; 2.2e-16 ***\nL_gas_price -0.420389   0.039978 -10.5155 &lt; 2.2e-16 ***\nL_cars_pc   -0.606840   0.025515 -23.7836 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    18.054\nResidual Sum of Squares: 3.0817\nR-Squared:      0.82931\nAdj. R-Squared: 0.8278\nChisq: 1642.2 on 3 DF, p-value: &lt; 2.22e-16\n\n\nThe comparison between the above the Pooled OLS is done by Breusch-Pagan LM Test.\n\nplmtest(random_1, type = \"bp\")\n\n\n    Lagrange Multiplier Test - (Breusch-Pagan)\n\ndata:  L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc\nchisq = 1465.6, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects\n\n\n\n\n12.1.8 Task 8\nApply the Hausman Test to compare Random Effects and Fixed Effects Models.\n\nphtest(fixed_1, random_1)\n\n\n    Hausman Test\n\ndata:  L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc\nchisq = 302.8, df = 3, p-value &lt; 2.2e-16\nalternative hypothesis: one model is inconsistent\n\n\nThe null of Random Effects is rejected. We choose the Fixed Effects.\n\n\n12.1.9 Task 9\nEstimate a two-way fixed effects model and test for the joint significance of the time effects.\n\n# Panel Data Fixed Effects Model\nfixed_2 &lt;- plm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + factor(year), \n               data = df, index = c(\"country\", \"year\"), model = \"within\")\nsummary(fixed_2)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + \n    factor(year), data = df, model = \"within\", index = c(\"country\", \n    \"year\"))\n\nBalanced Panel: n = 18, T = 19, N = 342\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-0.41920085 -0.03886111  0.00018502  0.04199566  0.23067839 \n\nCoefficients:\n                  Estimate Std. Error  t-value  Pr(&gt;|t|)    \nL_income_pc       0.051369   0.091386   0.5621 0.5744611    \nL_gas_price      -0.192850   0.042860  -4.4995 9.718e-06 ***\nL_cars_pc        -0.593448   0.027669 -21.4479 &lt; 2.2e-16 ***\nfactor(year)1961  0.040970   0.027248   1.5036 0.1337236    \nfactor(year)1962  0.044249   0.027595   1.6035 0.1098635    \nfactor(year)1963  0.064744   0.028277   2.2897 0.0227268 *  \nfactor(year)1964  0.105995   0.029297   3.6179 0.0003479 ***\nfactor(year)1965  0.124134   0.030049   4.1310 4.677e-05 ***\nfactor(year)1966  0.167830   0.031046   5.4058 1.310e-07 ***\nfactor(year)1967  0.198832   0.032048   6.2042 1.801e-09 ***\nfactor(year)1968  0.230077   0.033201   6.9299 2.537e-11 ***\nfactor(year)1969  0.242999   0.035304   6.8831 3.374e-11 ***\nfactor(year)1970  0.275080   0.037182   7.3982 1.365e-12 ***\nfactor(year)1971  0.304198   0.038516   7.8980 5.262e-14 ***\nfactor(year)1972  0.332136   0.040532   8.1944 7.160e-15 ***\nfactor(year)1973  0.369707   0.043209   8.5562 5.913e-16 ***\nfactor(year)1974  0.327938   0.042232   7.7652 1.266e-13 ***\nfactor(year)1975  0.362392   0.041877   8.6538 2.984e-16 ***\nfactor(year)1976  0.370891   0.043626   8.5016 8.651e-16 ***\nfactor(year)1977  0.385702   0.044559   8.6559 2.939e-16 ***\nfactor(year)1978  0.400956   0.046409   8.6397 3.295e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    17.061\nResidual Sum of Squares: 1.997\nR-Squared:      0.88295\nAdj. R-Squared: 0.86827\nF-statistic: 108.839 on 21 and 303 DF, p-value: &lt; 2.22e-16\n\n\n\n# fixed_1 : One-way fixed effects model (cross-section effects included)\n# fixed_2 : Two-way fixed effects model (cross-section and time-effects included)\n# Joint statistical significance of the time effects\npFtest(fixed_2, fixed_1)\n\n\n    F test for individual effects\n\ndata:  L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + factor(year)\nF = 6.2338, df1 = 18, df2 = 303, p-value = 5.36e-13\nalternative hypothesis: significant effects\n\n\nTime effects are jointly statistically significant. We choose the two-way FE model over one-way FE model.\nNote that we can estimate the above two-way model by adding an effect = \"twoways\" option in R, without the need for an explicit inclusion of time dummies.\n\n# Panel Data Fixed Effects Model\nfixed_3 &lt;- plm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n               data = df, index = c(\"country\", \"year\"), model = \"within\", effect = \"twoways\")\nsummary(fixed_3)\n\nTwoways effects Within Model\n\nCall:\nplm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n    data = df, effect = \"twoways\", model = \"within\", index = c(\"country\", \n        \"year\"))\n\nBalanced Panel: n = 18, T = 19, N = 342\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-0.41920085 -0.03886111  0.00018502  0.04199566  0.23067839 \n\nCoefficients:\n             Estimate Std. Error  t-value  Pr(&gt;|t|)    \nL_income_pc  0.051369   0.091386   0.5621    0.5745    \nL_gas_price -0.192850   0.042860  -4.4995 9.718e-06 ***\nL_cars_pc   -0.593448   0.027669 -21.4479 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    10.644\nResidual Sum of Squares: 1.997\nR-Squared:      0.81239\nAdj. R-Squared: 0.78886\nF-statistic: 437.338 on 3 and 303 DF, p-value: &lt; 2.22e-16\n\n\n\n\n12.1.10 Task 10\nEstimate a two-way random effects model.\n\n# Panel Data Fixed Effects Model\nrandom_2 &lt;- plm(L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n               data = df, index = c(\"country\", \"year\"), \n               model = \"random\", effect = \"twoways\")\nsummary(random_2)\n\nTwoways effects Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc, \n    data = df, effect = \"twoways\", model = \"random\", index = c(\"country\", \n        \"year\"))\n\nBalanced Panel: n = 18, T = 19, N = 342\n\nEffects:\n                   var  std.dev share\nidiosyncratic 0.006591 0.081183 0.147\nindividual    0.038340 0.195805 0.853\ntime          0.000000 0.000000 0.000\ntheta: 0.9053 (id) 0 (time) 0 (total)\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.3956900 -0.0499700  0.0075613  0.0543043  0.3748214 \n\nCoefficients:\n             Estimate Std. Error  z-value  Pr(&gt;|z|)    \n(Intercept)  2.040793   0.191508  10.6564 &lt; 2.2e-16 ***\nL_income_pc  0.564562   0.060854   9.2773 &lt; 2.2e-16 ***\nL_gas_price -0.404936   0.040369 -10.0309 &lt; 2.2e-16 ***\nL_cars_pc   -0.609360   0.025970 -23.4641 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    17.829\nResidual Sum of Squares: 3.0145\nR-Squared:      0.83092\nAdj. R-Squared: 0.82942\nChisq: 1661.05 on 3 DF, p-value: &lt; 2.22e-16\n\n\n\n\n12.1.11 Task 11\nPerform a Hausman Test comparing two-way fixed and random effect models.\n\nphtest(fixed_2, random_2)\n\n\n    Hausman Test\n\ndata:  L_gas_cons_pcar ~ L_income_pc + L_gas_price + L_cars_pc + factor(year)\nchisq = 134.07, df = 3, p-value &lt; 2.2e-16\nalternative hypothesis: one model is inconsistent",
    "crumbs": [
      "Seminar 7 (4 March 2025)",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Panel Data Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "James, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2023.\nAn Introduction to Statistical Learning. 2nd edition. Springer.\nhttps://www.statlearning.com.\n\n\nKleiber, C., and A. Zeileis. 2008. Applied Econometrics with r.\nSpringer.\n\n\nRiegler, Robert. 2022. “R Workbook - Guidance for\nWorksheets.” Aston University.\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. n.d.\nR for Data Science. 2nd edition. O’Reilly. https://r4ds.hadley.nz/preface-2e.\n\n\nWilson, J. H., and B. Keating. 2007. Business Forecasting. 5th\nedition. McGraw-Hill.",
    "crumbs": [
      "References",
      "References"
    ]
  }
]